{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zahra-FallahMMA/DRL_Offload_Allocation/blob/main/Random_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcuUUZU1G5k4"
      },
      "source": [
        "# import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIP4GlsbGwAx",
        "outputId": "c39ca4d9-ecf3-46d0-e714-4e3711bef37a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpy in /usr/local/lib/python3.11/dist-packages (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from io import StringIO\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import deque, defaultdict\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from itertools import product\n",
        "import tensorflow as tf\n",
        "!pip install simpy\n",
        "import simpy\n",
        "\n",
        "# Set TensorFlow logging level to suppress detailed logs\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCfIwrJDHK33"
      },
      "source": [
        "# Class Random Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qreTNfv_HP9X"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "\n",
        "   def __init__(self, action_size):\n",
        "      self.action_size = action_size\n",
        "\n",
        "   def choose_action(self):\n",
        "        return random.randrange(self.action_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc5cwMB6HUAZ"
      },
      "source": [
        "# class Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dWqg63i_HW5y"
      },
      "outputs": [],
      "source": [
        "class Task:\n",
        "    def __init__(self, id, instructions, workflow_id):\n",
        "        self.id = id\n",
        "        self.instructions = instructions  # Execution time or computational instructions\n",
        "        self.children = []  # List of tasks that depend on this task\n",
        "        self.parents = []  # List of tasks this task depends on\n",
        "\n",
        "        self.input_files = []   # list of (filename, size)\n",
        "        self.output_files = []  # list of (filename, size)\n",
        "\n",
        "        self.executed = False  # Status of execution\n",
        "        self.executed_on = None  # Node this task was executed on\n",
        "        self.execution_time = 0  # Time taken to execute the task\n",
        "        self.cost = 0  # Cost of executing the task\n",
        "        self.comm_delay = 0  # Communication delay in seconds\n",
        "        self.workflow_id = workflow_id  # Workflow identifier to which this task belongs\n",
        "        self.total_transfer_files_size = 0\n",
        "        self.waiting_time = 0\n",
        "        self.total_file_size = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6GeROhDHben"
      },
      "source": [
        "# Class Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "l1dvEPosHyYn"
      },
      "outputs": [],
      "source": [
        "class Workflow:\n",
        "    def __init__(self, id):\n",
        "        self.id = id  # Workflow identifier\n",
        "        self.tasks = {}  # Dictionary of tasks in the workflow\n",
        "\n",
        "    def add_task(self, task_id, instructions, parent_ids=[], input_files=[], output_files=[]):\n",
        "        if task_id not in self.tasks:\n",
        "            self.tasks[task_id] = Task(task_id, instructions, self.id)\n",
        "        task = self.tasks[task_id]\n",
        "        for parent_id in parent_ids:\n",
        "            if parent_id not in self.tasks:\n",
        "                self.tasks[parent_id] = Task(parent_id, 0, self.id)\n",
        "            parent_task = self.tasks[parent_id]\n",
        "            parent_task.children.append(task)\n",
        "            task.parents.append(parent_task)\n",
        "\n",
        "        task.input_files.extend(input_files)\n",
        "        task.output_files.extend(output_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOhKq478H4_X"
      },
      "source": [
        "# class parse_dax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "92rkJc8AH3Bd"
      },
      "outputs": [],
      "source": [
        "def parse_dax(file_path, workflow_id):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    workflow_id = workflow_id\n",
        "    workflow = Workflow(workflow_id)\n",
        "\n",
        "    # Pegasus DAX namespace\n",
        "    dax_ns = '{http://pegasus.isi.edu/schema/DAX}'\n",
        "\n",
        "    # Parse jobs\n",
        "    jobs = {job.attrib['id']: job for job in root.findall(f'{dax_ns}job')}\n",
        "\n",
        "    # Add jobs to workflow\n",
        "    for job_id, job_elem in jobs.items():\n",
        "        instructions = float(job_elem.attrib.get('runtime', 0))\n",
        "        stats[\"task.instructions_max\"] = max(instructions, stats[\"task.instructions_max\"])\n",
        "        stats[\"task.instructions_min\"] = min(instructions, stats[\"task.instructions_min\"])\n",
        "\n",
        "        # NEW: Gather input/output files from <uses link=\"input\" ...> or <uses link=\"output\" ...>\n",
        "        input_files = []\n",
        "        output_files = []\n",
        "        for uses_elem in job_elem.findall(f'.//{dax_ns}uses'):\n",
        "            link_type = uses_elem.attrib.get('link', '')\n",
        "            file_name = uses_elem.attrib.get('file', '')\n",
        "            file_size_str = uses_elem.attrib.get('size', '0')\n",
        "            file_size = int(file_size_str)\n",
        "\n",
        "            if link_type == 'input':\n",
        "                input_files.append((file_name, file_size))  # e.g. (\"FFI_0_2_subfx.sgt\", 262360916)\n",
        "            elif link_type == 'output':\n",
        "                output_files.append((file_name, file_size))\n",
        "\n",
        "        workflow.add_task(job_id,\n",
        "                          instructions,\n",
        "                          parent_ids=[],  # We'll handle parents in a moment,\n",
        "                          input_files=input_files,\n",
        "                          output_files=output_files)\n",
        "\n",
        "    # Parse dependencies\n",
        "    for child in root.findall(f'{dax_ns}child'):\n",
        "        child_id = child.attrib['ref']\n",
        "        parent_ids = [parent.attrib['ref'] for parent in child.findall(f'{dax_ns}parent')]\n",
        "        workflow.add_task(child_id, 0, parent_ids)  # Adds a child node with its parent nodes, setting instructions to 0 to avoid overwrite\n",
        "\n",
        "    return workflow\n",
        "\n",
        "\n",
        "def ensemble_of_workflows(name, size=10, distribution='constant', dax_path=''):\n",
        "    ws = []\n",
        "    ensemble = []\n",
        "    directory_path = dax_path  # Directory containing DAX files\n",
        "\n",
        "    # List and filter files in directory\n",
        "    files = os.listdir(directory_path)\n",
        "    filtered_files = [file for file in files if name in file]\n",
        "\n",
        "    if distribution == 'constant':\n",
        "        pattern = r'100(?!\\d)'\n",
        "        for s in filtered_files:\n",
        "            if re.search(pattern, s):\n",
        "                ensemble = [s] * size  # Replicate the matched file 'size' times\n",
        "                break\n",
        "    else:\n",
        "        numbers = np.random.randint(0, len(filtered_files), size)\n",
        "        #print('numbers: ', numbers)\n",
        "        ensemble = [filtered_files[i] for i in numbers]  # Select random files based on uniform distribution\n",
        "\n",
        "    w_id = 0\n",
        "    for name in ensemble:\n",
        "        ws.append(parse_dax(dax_path+name,w_id))\n",
        "        w_id = w_id + 1\n",
        "\n",
        "    return ws"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8oPYhPOIF39"
      },
      "source": [
        "# Loading dax files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z69MgkfSH81N",
        "outputId": "a45e2e83-00dd-4816-8557-883645bd39b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import glob\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/My Drive/Zahra/dax/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x5Y4MN-IMLe"
      },
      "source": [
        "# class Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kK7fXmQeIPIW"
      },
      "outputs": [],
      "source": [
        "class Device:\n",
        "    def __init__(self, id, mips, cost_per_hour, env):\n",
        "        self.id = id\n",
        "        self.mips = mips\n",
        "        self.cost_per_hour = cost_per_hour\n",
        "        self.queue = deque()\n",
        "        self.runnig_queue = deque()\n",
        "        self.resource = simpy.Resource(env, capacity=1)\n",
        "        self.dev_type = id.split('_')[0]\n",
        "\n",
        "    def add_task_to_queue(self, task):\n",
        "        self.queue.append(task)\n",
        "\n",
        "    def get_next_task(self):\n",
        "        return self.queue.popleft() if self.queue else None\n",
        "\n",
        "    def waiting_time(self):\n",
        "        waiting_time = 0\n",
        "        for t in self.queue:\n",
        "            waiting_time += t.instructions / self.mips\n",
        "        return waiting_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "QzHKAtl-maMp"
      },
      "outputs": [],
      "source": [
        "stats = {\n",
        "    \"self.cost_max\" : 0, \"self.cost_min\":0,\n",
        "    \"task.instructions_max\" : 0, \"task.instructions_min\":100000000,\n",
        "    \"task.cost_max\" : 0, \"task.cost_min\":10000000,\n",
        "    \"task.total_transfer_files_size_max\" : 0, \"task.total_transfer_files_size_min\":100000000,\n",
        "    \"task.storage_cost_max\" : 0, \"task.storage_cost_min\":100000000,\n",
        "    \"self.total_delay_max\" : 0, \"self.total_delay_min\":10000000,\n",
        "    \"task.execution_time_max\" : 0, \"task.execution_time_min\":10000000,\n",
        "    \"task.comm_delay_max\" : 0, \"task.comm_delay_min\":1000000,\n",
        "    \"d.waiting_time_max\" : 0, \"d.waiting_time_min\":10000000,\n",
        "    \"task.total_files_size_max\" : 0, \"task.total_files_size_min\":100000000\n",
        "}\n",
        "\n",
        "def format_dict_as_code(d, var_name=\"stats\"):\n",
        "    result = f\"{var_name} = {{\"\n",
        "    items = list(d.items())\n",
        "    for i in range(0, len(items), 2):\n",
        "        k1, v1 = items[i]\n",
        "        line = f'    \"{k1}\" : {v1}'\n",
        "        if i + 1 < len(items):\n",
        "            k2, v2 = items[i + 1]\n",
        "            line += f', \"{k2}\":{v2},'\n",
        "        result += line\n",
        "    result += \"}\"\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUV48C7AImGY"
      },
      "source": [
        "# class FogEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "A8B_f_ZNIovb"
      },
      "outputs": [],
      "source": [
        "import simpy\n",
        "\n",
        "class FogEnv:\n",
        "    def __init__(self, env, iot_devices, fog_nodes, cloud_servers, workflows):\n",
        "        self.env = env\n",
        "        self.iot_devices = iot_devices\n",
        "        self.fog_nodes = fog_nodes\n",
        "        self.cloud_servers = cloud_servers\n",
        "        self.cost = 0\n",
        "        self.completed_workflows = 0\n",
        "        self.workflows = workflows\n",
        "        self.total_delay = 0\n",
        "\n",
        "        # NEW: Track file origins\n",
        "        self.file_origins = {}  # { file_name: device_id }\n",
        "\n",
        "        # Example bandwidth table (Mb/s)\n",
        "        self.bandwidth_matrix = {\n",
        "            ('iot', 'iot'): 2000,\n",
        "            ('iot', 'fog'): 500,\n",
        "            ('iot', 'cloud'): 500,\n",
        "            ('fog', 'iot'): 500,\n",
        "            ('fog', 'fog'): 4000,\n",
        "            ('fog', 'cloud'): 1000,\n",
        "            ('cloud', 'iot'): 500,\n",
        "            ('cloud', 'fog'): 1000,\n",
        "            ('cloud', 'cloud'): 1000\n",
        "        }\n",
        "\n",
        "\n",
        "        # Storage cost rate per GB per hour for each device type (in dollars)\n",
        "        self.storage_cost_rate = {\n",
        "            'iot': 0.00007,  # $ per GB per hour for IoT devices\n",
        "            'fog': 0.000042,  # $ per GB per hour for Fog nodes\n",
        "            'cloud': 0.000003  # $ per GB per hour for Cloud servers\n",
        "        }\n",
        "\n",
        "\n",
        "        self.transfer_cost_rate = {\n",
        "            ('iot', 'fog'): 0.00005, ('iot', 'cloud'): 0.0002,\n",
        "            ('fog', 'cloud'): 0.0001, ('fog', 'iot'): 0.00005,\n",
        "            ('cloud', 'fog'): 0.0001, ('cloud', 'iot'): 0.0002,\n",
        "        }\n",
        "\n",
        "\n",
        "    def estimate_transfer_cost(self, task, device):\n",
        "        total_transfer_cost = 0\n",
        "        for file_name, file_size in task.input_files:\n",
        "            src_id = self.file_origins.get(file_name, device.id)\n",
        "            src_dev = self.get_device_by_id(src_id)\n",
        "            if src_dev and src_dev.id != device.id:\n",
        "                rate = self.transfer_cost_rate.get((src_dev.dev_type, device.dev_type), 0.0001)\n",
        "                total_transfer_cost += file_size / (1024**2) * rate  # in MB\n",
        "        return total_transfer_cost\n",
        "\n",
        "\n",
        "\n",
        "    def assign_task(self, task, device):\n",
        "        with device.resource.request() as req:\n",
        "            yield req\n",
        "\n",
        "            # Include tasks already running or queued\n",
        "            total_instructions = sum(t.instructions for t in device.runnig_queue)  # Currently running\n",
        "            total_instructions += sum(t.instructions for t in device.queue)       # Waiting\n",
        "\n",
        "            waiting_time = (total_instructions * 6000) / device.mips\n",
        "\n",
        "            # 1) Transfer each input file if needed\n",
        "            for file_name, file_size_bytes in task.input_files:\n",
        "                yield self.env.process(self.handle_file_transfer(task, file_name, file_size_bytes, device))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # 2) Execute the task on this device\n",
        "            execution_time = task.instructions * 6000/ device.mips\n",
        "            self.total_delay += execution_time\n",
        "            task.execution_time = execution_time\n",
        "            yield self.env.timeout(execution_time)\n",
        "\n",
        "            # 3) Calculate waiting time (difference between current time and arrival time)\n",
        "\n",
        "\n",
        "            # 4) Update cost\n",
        "            task.cost = execution_time * device.cost_per_hour\n",
        "            self.cost += task.cost\n",
        "\n",
        "            stats[\"task.cost_max\"] = max(task.cost, stats[\"task.cost_max\"])\n",
        "            stats[\"task.cost_min\"] = min(task.cost, stats[\"task.cost_min\"])\n",
        "\n",
        "            # 5) Calculate and add storage cost (including waiting time)\n",
        "            storage_cost = self.calculate_storage_cost(task, device, waiting_time, execution_time)\n",
        "            task.storage_cost = storage_cost\n",
        "            #self.cost += storage_cost\n",
        "\n",
        "            #self.cost+= self.estimate_transfer_cost(task, device)\n",
        "\n",
        "            stats[\"self.cost_max\"] = max(self.cost, stats[\"self.cost_max\"])\n",
        "\n",
        "            stats[\"task.storage_cost_max\"] = max(task.storage_cost, stats[\"task.storage_cost_max\"])\n",
        "            stats[\"task.storage_cost_min\"] = min(task.storage_cost, stats[\"task.storage_cost_min\"])\n",
        "\n",
        "            task.executed = True\n",
        "            task.executed_on = device.id\n",
        "            task.execution_time = execution_time\n",
        "\n",
        "            stats[\"task.execution_time_max\"] = max(task.execution_time, stats[\"task.execution_time_max\"] )\n",
        "            stats[\"task.execution_time_min\"] = max(task.execution_time, stats[\"task.execution_time_min\"] )\n",
        "\n",
        "            # 6) Mark output files as originating from this device\n",
        "            for out_file, _sz in task.output_files:\n",
        "                self.file_origins[out_file] = device.id\n",
        "\n",
        "            # 7) Check if this workflow is now complete\n",
        "            wf = next(w for w in self.workflows if w.id == task.workflow_id)\n",
        "            self.check_workflow_completion(wf)\n",
        "            device.queue.popleft()\n",
        "\n",
        "\n",
        "    def calculate_storage_cost(self, task, device, waiting_time, execution_time):\n",
        "        total_file_size = 0\n",
        "\n",
        "        # Add the size of input files\n",
        "        for file_name, file_size_bytes in task.input_files:\n",
        "            total_file_size += file_size_bytes\n",
        "\n",
        "        # Add the size of output files\n",
        "        for out_file, file_size_bytes in task.output_files:\n",
        "            total_file_size += file_size_bytes\n",
        "\n",
        "        stats[\"task.total_files_size_max\"] = max(total_file_size, stats[\"task.total_files_size_max\"])\n",
        "        stats[\"task.total_file_size_min\"] = min(total_file_size, stats[\"task.total_files_size_min\"])\n",
        "\n",
        "        # Convert total size to GB\n",
        "        total_file_size_gb = total_file_size / (1024 ** 3)  # Convert bytes to GB\n",
        "\n",
        "        # Get the storage cost rate for the device type\n",
        "        device_type = device.dev_type\n",
        "        storage_cost_per_hour = self.storage_cost_rate.get(device_type, 0)\n",
        "\n",
        "        # Calculate the total storage time (waiting time + execution time)\n",
        "        total_storage_time = (waiting_time + execution_time)\n",
        "\n",
        "        # Convert total time from seconds to hours\n",
        "        total_time_in_hours = total_storage_time / 3600  # Convert seconds to hours\n",
        "\n",
        "\n",
        "        # Calculate the storage cost for this task (based on total storage time)\n",
        "        storage_cost = total_file_size_gb * storage_cost_per_hour * total_storage_time  # cost per GB per hour * time\n",
        "        return storage_cost\n",
        "\n",
        "\n",
        "    # Function to handle transfer\n",
        "    def handle_file_transfer(self,task, file_name, file_size_bytes, destination_device):\n",
        "\n",
        "\n",
        "        # If we don't know the file origin, assume it's \"local\" or from some default place\n",
        "        if file_name not in self.file_origins:\n",
        "            return  # no transfer needed\n",
        "\n",
        "        source_dev_id = self.file_origins[file_name]\n",
        "        if source_dev_id == destination_device.id:\n",
        "            return  # already local\n",
        "\n",
        "        # Get device types for bandwidth lookup\n",
        "        source_dev = self.get_device_by_id(source_dev_id)\n",
        "        if source_dev is None:\n",
        "            return  # fallback: no known device\n",
        "\n",
        "        task.total_transfer_files_size += file_size_bytes\n",
        "\n",
        "        stats[\"task.total_transfer_files_size_max\"] = max(task.total_transfer_files_size, stats[\"task.total_transfer_files_size_max\"])\n",
        "        stats[\"task.total_transfer_files_size_min\"] = min(task.total_transfer_files_size, stats[\"task.total_transfer_files_size_min\"])\n",
        "\n",
        "        source_type = source_dev.dev_type\n",
        "        dest_type = destination_device.dev_type\n",
        "\n",
        "        # bandwidth_matrix gives us Mb/s. Convert bytes -> megabits\n",
        "        # 1 byte = 8 bits, so:\n",
        "        file_megabits = file_size_bytes * 8 / 1e6\n",
        "\n",
        "        # Look up or fallback\n",
        "        band_mbps = self.bandwidth_matrix.get((source_type, dest_type), 1000.0)\n",
        "\n",
        "        # Transfer time (seconds)\n",
        "        transfer_time = file_megabits / band_mbps\n",
        "\n",
        "        # Optional: add base communication latency\n",
        "        comm_latency = 0.01  # 10 ms, for example\n",
        "        total_delay = comm_latency + transfer_time\n",
        "        self.total_delay += total_delay\n",
        "        task.comm_delay = total_delay\n",
        "\n",
        "        stats[\"self.total_delay_max\"] = max(self.total_delay, stats[\"self.total_delay_max\"])\n",
        "        stats[\"self.total_delay_min\"] = min(self.total_delay, stats[\"self.total_delay_min\"])\n",
        "\n",
        "        stats[\"task.comm_delay_max\"] = max(task.comm_delay,  stats[\"task.comm_delay_max\"])\n",
        "        stats[\"task.comm_delay_min\"] = max(task.comm_delay,  stats[\"task.comm_delay_min\"])\n",
        "\n",
        "\n",
        "\n",
        "        # Wait out the transfer in simulation time\n",
        "        yield self.env.timeout(total_delay)\n",
        "\n",
        "    def get_device_by_id(self, device_id):\n",
        "        for device in self.iot_devices + self.fog_nodes + self.cloud_servers:\n",
        "            if device.id == device_id:\n",
        "                return device\n",
        "        return None\n",
        "\n",
        "    def check_workflow_completion(self, workflow):\n",
        "        # Check if all tasks in the workflow are executed\n",
        "        if all(task.executed for task in workflow.tasks.values()):\n",
        "            self.completed_workflows += 1  # Increment completed workflows counter\n",
        "            # print(f\"Workflow {workflow.id} is completed! Total completed workflows: {self.completed_workflows}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_workflow(env, workflow, fog_env, agent):\n",
        "    # A list to hold all the task processes (concurrent tasks)\n",
        "\n",
        "    while(True):\n",
        "        if all([task.executed for task in workflow.tasks.values()]):\n",
        "          break\n",
        "\n",
        "        processes = []\n",
        "        # Iterate through each task in the workflow\n",
        "        for task in workflow.tasks.values():\n",
        "            if task.executed:\n",
        "                continue\n",
        "\n",
        "            # Check if the task can be executed (all parent tasks are done)\n",
        "            if all([parent.executed for parent in task.parents]) or task.parents == []:\n",
        "                # Choose the action based on the current state (device selection)\n",
        "                action = agent.choose_action()\n",
        "\n",
        "                # Get the list of devices (IoT devices, fog nodes, and cloud servers)\n",
        "                devices = fog_env.iot_devices + fog_env.fog_nodes + fog_env.cloud_servers\n",
        "                device = devices[action]\n",
        "\n",
        "                # Add the task to the device's queue\n",
        "                device.add_task_to_queue(task)\n",
        "                task.executed_on = device.id\n",
        "\n",
        "                # Create a process for the task and add it to the processes list\n",
        "                process = env.process(fog_env.assign_task(task, device))\n",
        "                processes.append(process)  # Track the process\n",
        "\n",
        "        # After all tasks are started, wait for all of them to finish\n",
        "        yield env.all_of(processes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swj5rrsAJZV8"
      },
      "source": [
        "# class NewSim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "SB2sfq22JcBF"
      },
      "outputs": [],
      "source": [
        "class  NewSim:\n",
        "    def __init__(self, num_iot, num_fog, num_server, learning_rate=0.001, discount_factor=0.95,\n",
        "                 exploration_rate=1.0, exploration_decay=0.995, exploration_min=0.01, batch_size=64, memory_size=2000, model_path=None):\n",
        "        self.num_iot = num_iot\n",
        "        self.num_fog = num_fog\n",
        "        self.num_server = num_server\n",
        "        self.num_totla_dev = num_iot + num_fog + num_server\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_min = exploration_min\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_size = memory_size\n",
        "        self.model_path = model_path\n",
        "        self.env = simpy.Environment()\n",
        "        self.reset()\n",
        "        self.run()\n",
        "\n",
        "    def reset(self):\n",
        "        self.iot_devices = [Device(f'iot_{i}', 500, 0, self.env) for i in range(self.num_iot)]\n",
        "        self.fog_devices = [Device(f'fog_{i}', 4000, 1/3600, self.env) for i in range(self.num_fog)]\n",
        "        self.server_devices = [Device(f'server_{i}', 6000, 8/3600, self.env) for i in range(self.num_server)]\n",
        "        self.agent = RandomAgent(action_size=self.num_totla_dev)\n",
        "\n",
        "        self.workflows = ensemble_of_workflows(name = 'CyberShake', size=100, distribution = 'constant', dax_path=\"/content/drive/My Drive/Zahra/dax/\")\n",
        "    def run(self):\n",
        "        self.fog_env = FogEnv(self.env, self.iot_devices, self.fog_devices, self.server_devices,self.workflows)\n",
        "        for workflow in self.workflows:\n",
        "            self.env.process(process_workflow(self.env, workflow, self.fog_env, self.agent))\n",
        "\n",
        "        self.env.run()  # Run simulation for a time period\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m16rQswH07A"
      },
      "source": [
        "# Data Collection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRiUawjwavRO"
      },
      "source": [
        "## The Config of Workflows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "pi6ASMtxau1Y"
      },
      "outputs": [],
      "source": [
        "# Workflow configurations with specific sizes\n",
        "workflow_configs = {\n",
        "        'CyberShake': [30, 50, 100, 1000],\n",
        "        'Montage': [20, 40, 60, 80, 100, 200, 300],\n",
        "        'Inspiral': [30, 50, 100, 1000],\n",
        "        'Sipht': [29, 58, 100, 968]\n",
        "    }\n",
        "workflow_distributions = ['constant', 'uniform']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "BrXkI2OeLjOO"
      },
      "outputs": [],
      "source": [
        "def run_simulation_with_results_tracking(workflow_name, workflow_size, workflow_distribution, model_path):\n",
        "    learning_rate = 0.001\n",
        "    discount_factor = 0.99\n",
        "    exploration_rate = 0.5\n",
        "    exploration_decay = 0.995\n",
        "    exploration_min = 0.05\n",
        "\n",
        "    dax_path = \"/content/drive/My Drive/Zahra/dax/\"\n",
        "\n",
        "    print(f\"Running simulation for Workflow: {workflow_name}, Size: {workflow_size}, Distribution: {workflow_distribution}\")\n",
        "\n",
        "    # Set up the simulation with the current parameters\n",
        "    simulation = NewSim(\n",
        "        num_iot=10,\n",
        "        num_fog=8,\n",
        "        num_server=5,\n",
        "        learning_rate=learning_rate,\n",
        "        discount_factor=discount_factor,\n",
        "        exploration_rate=exploration_rate,\n",
        "        exploration_decay=exploration_decay,\n",
        "        exploration_min=exploration_min,\n",
        "        model_path = model_path\n",
        "    )\n",
        "\n",
        "    # Update the workflow parameters\n",
        "    simulation.workflows = ensemble_of_workflows(\n",
        "        name=workflow_name,\n",
        "        size=workflow_size,\n",
        "        distribution=workflow_distribution,\n",
        "        dax_path=dax_path\n",
        "    )\n",
        "\n",
        "    # Run the simulation\n",
        "    simulation.run()\n",
        "\n",
        "    # Print results for current run\n",
        "    print(f\"Total cost for Workflow {workflow_name} ({workflow_distribution}, size={workflow_size}): {simulation.fog_env.cost}\")\n",
        "    print(f\"Total time for Workflow {workflow_name} ({workflow_distribution}, size={workflow_size}): {simulation.env.now}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLU8FKZT2elj"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XeITaLMw79l"
      },
      "source": [
        "## Cybershake - constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6NHXFqyw2Oz",
        "outputId": "6171dd85-929f-48e4-8a98-78210fef3192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running simulation for Workflow: CyberShake, Size: 10, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=10): 21.525023611111077\n",
            "Total time for Workflow CyberShake (constant, size=10): 212925.56881419255\n",
            "Running simulation for Workflow: CyberShake, Size: 20, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=20): 42.288922222222105\n",
            "Total time for Workflow CyberShake (constant, size=20): 231235.5096972488\n",
            "Running simulation for Workflow: CyberShake, Size: 30, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=30): 58.620730555555525\n",
            "Total time for Workflow CyberShake (constant, size=30): 251968.47627489682\n",
            "Running simulation for Workflow: CyberShake, Size: 40, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=40): 76.610788888889\n",
            "Total time for Workflow CyberShake (constant, size=40): 265578.70217786456\n",
            "Running simulation for Workflow: CyberShake, Size: 50, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=50): 100.2563722222229\n",
            "Total time for Workflow CyberShake (constant, size=50): 286106.2724639127\n",
            "Running simulation for Workflow: CyberShake, Size: 60, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=60): 120.82938750000064\n",
            "Total time for Workflow CyberShake (constant, size=60): 293463.57203847263\n",
            "Running simulation for Workflow: CyberShake, Size: 70, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=70): 140.64844583333547\n",
            "Total time for Workflow CyberShake (constant, size=70): 314134.8626588802\n",
            "Running simulation for Workflow: CyberShake, Size: 80, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=80): 160.28863888889006\n",
            "Total time for Workflow CyberShake (constant, size=80): 324834.07658279984\n",
            "Running simulation for Workflow: CyberShake, Size: 90, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=90): 183.13331250000246\n",
            "Total time for Workflow CyberShake (constant, size=90): 372687.02026691264\n",
            "Running simulation for Workflow: CyberShake, Size: 100, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=100): 207.02363750000225\n",
            "Total time for Workflow CyberShake (constant, size=100): 385349.8426279335\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 11):\n",
        "  run_simulation_with_results_tracking('CyberShake',10*i, 'constant', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "Sdoa-IcrAZWt",
        "outputId": "579c80b2-b19a-440e-e7aa-84f6bf4d50ba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'stats = {    \"self.cost_max\" : 38712.27163848927, \"self.cost_min\":0,    \"task.instructions_max\" : 203.78, \"task.instructions_min\":0.34,    \"task.cost_max\" : 0.45284444444444444, \"task.cost_min\":0.0,    \"task.total_transfer_files_size_max\" : 0, \"task.total_transfer_files_size_min\":100000000,    \"task.storage_cost_max\" : 129.6453880033913, \"task.storage_cost_min\":0.0,    \"self.total_delay_max\" : 0, \"self.total_delay_min\":10000000,    \"task.execution_time_max\" : 2445.36, \"task.execution_time_min\":10000000,    \"task.comm_delay_max\" : 0, \"task.comm_delay_min\":1000000,    \"d.waiting_time_max\" : 0, \"d.waiting_time_min\":10000000,    \"task.total_files_size_max\" : 41510774022, \"task.total_files_size_min\":100000000,    \"task.total_file_size_min\" : 100000000}'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "format_dict_as_code(stats, var_name=\"stats\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR0TdNcs1qHQ"
      },
      "source": [
        "## Cybershake - uniform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDrLPm5E2BMz",
        "outputId": "cba66b05-f236-45bf-de73-8430a7cf3bd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running simulation for Workflow: CyberShake, Size: 10, Distribution: uniform\n",
            "numbers:  [2 3 2 3 3 1 2 3 0 3]\n",
            "Total cost for Workflow CyberShake (uniform, size=10): 2543.02241201375\n",
            "Total time for Workflow CyberShake (uniform, size=10): 85962.71999999999\n",
            "Running simulation for Workflow: CyberShake, Size: 20, Distribution: uniform\n",
            "numbers:  [3 0 1 4 2 2 2 3 2 0 4 1 1 0 0 0 4 2 4 4]\n",
            "Total cost for Workflow CyberShake (uniform, size=20): 3420.0959876652405\n",
            "Total time for Workflow CyberShake (uniform, size=20): 87861.0\n",
            "Running simulation for Workflow: CyberShake, Size: 30, Distribution: uniform\n",
            "numbers:  [2 4 3 3 2 4 0 2 2 2 1 0 2 2 1 3 2 2 0 0 0 3 4 0 2 3 2 1 3 0]\n",
            "Total cost for Workflow CyberShake (uniform, size=30): 6681.806287570652\n",
            "Total time for Workflow CyberShake (uniform, size=30): 111775.79999999999\n",
            "Running simulation for Workflow: CyberShake, Size: 40, Distribution: uniform\n",
            "numbers:  [4 4 2 1 0 4 4 3 3 3 2 4 1 2 4 1 4 2 4 4 3 1 1 4 4 0 3 1 3 4 3 0 1 1 4 4 3\n",
            " 0 4 0]\n",
            "Total cost for Workflow CyberShake (uniform, size=40): 7976.950913127303\n",
            "Total time for Workflow CyberShake (uniform, size=40): 92638.67999999998\n",
            "Running simulation for Workflow: CyberShake, Size: 50, Distribution: uniform\n",
            "numbers:  [2 4 2 2 0 4 3 3 4 3 2 0 0 3 3 4 3 0 4 2 2 0 4 2 4 2 1 0 4 0 1 0 2 2 2 4 3\n",
            " 0 0 2 2 3 2 0 3 2 1 0 3 4]\n",
            "Total cost for Workflow CyberShake (uniform, size=50): 12590.46195818704\n",
            "Total time for Workflow CyberShake (uniform, size=50): 115778.4\n",
            "Running simulation for Workflow: CyberShake, Size: 60, Distribution: uniform\n",
            "numbers:  [1 3 0 4 3 4 3 3 2 1 0 3 1 0 1 4 4 1 2 0 2 1 0 0 1 4 0 2 2 0 1 1 0 4 2 4 1\n",
            " 1 1 1 1 0 1 3 4 2 3 1 1 0 3 4 3 2 3 2 0 0 1 1]\n",
            "Total cost for Workflow CyberShake (uniform, size=60): 11818.19046348126\n",
            "Total time for Workflow CyberShake (uniform, size=60): 102946.20000000003\n",
            "Running simulation for Workflow: CyberShake, Size: 70, Distribution: uniform\n",
            "numbers:  [2 0 3 0 0 0 1 1 1 3 2 1 2 4 2 1 3 3 0 4 1 4 2 2 0 3 1 0 2 1 0 1 1 2 2 3 3\n",
            " 3 3 3 4 1 4 0 3 3 4 4 2 2 2 1 4 1 4 4 2 1 4 4 1 2 2 2 3 0 0 0 2 3]\n",
            "Total cost for Workflow CyberShake (uniform, size=70): 13904.705406330022\n",
            "Total time for Workflow CyberShake (uniform, size=70): 110738.16\n",
            "Running simulation for Workflow: CyberShake, Size: 80, Distribution: uniform\n",
            "numbers:  [1 0 4 1 2 2 3 3 3 2 3 3 2 0 2 0 4 4 3 3 4 3 2 2 4 1 3 2 0 1 2 2 4 4 4 1 1\n",
            " 2 3 0 3 3 4 4 2 4 2 3 2 3 0 3 1 1 3 1 0 4 0 4 1 2 1 0 0 0 1 2 4 1 4 1 0 2\n",
            " 1 2 0 4 2 0]\n",
            "Total cost for Workflow CyberShake (uniform, size=80): 17198.949267892098\n",
            "Total time for Workflow CyberShake (uniform, size=80): 134756.99999999997\n",
            "Running simulation for Workflow: CyberShake, Size: 90, Distribution: uniform\n",
            "numbers:  [2 0 1 0 0 1 4 1 0 2 4 1 1 0 4 1 0 3 1 3 4 4 4 0 3 1 1 0 0 2 3 1 3 3 1 4 2\n",
            " 0 1 4 4 1 4 2 2 3 2 4 3 4 2 1 2 2 3 1 4 3 4 1 0 0 1 0 4 3 2 0 0 4 0 3 1 3\n",
            " 0 3 0 1 4 4 3 2 2 2 3 2 2 2 1 3]\n",
            "Total cost for Workflow CyberShake (uniform, size=90): 18949.268461074185\n",
            "Total time for Workflow CyberShake (uniform, size=90): 119179.08\n",
            "Running simulation for Workflow: CyberShake, Size: 100, Distribution: uniform\n",
            "numbers:  [2 3 4 4 0 1 4 3 0 1 0 4 4 0 0 3 3 1 2 1 4 1 2 1 1 1 0 3 1 4 1 1 0 4 0 3 4\n",
            " 0 0 0 2 0 0 2 3 2 0 3 0 3 3 2 0 1 2 2 2 1 3 0 4 4 3 2 1 2 3 0 4 3 3 4 3 3\n",
            " 1 3 3 3 1 2 1 0 3 2 2 4 1 4 4 4 0 1 3 3 1 1 0 1 2 4]\n",
            "Total cost for Workflow CyberShake (uniform, size=100): 20922.998788363097\n",
            "Total time for Workflow CyberShake (uniform, size=100): 144140.27999999994\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 11):\n",
        "  run_simulation_with_results_tracking('CyberShake',10*i, 'uniform', '/content/drive/My Drive/Zahra/Models/model_CyberShake_constant_10_cost.keras' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmy3_T952Yuk"
      },
      "source": [
        "## Montage - constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1i-P0kL2pjV",
        "outputId": "1df6bd0a-77c2-444c-d8fb-b1c145e0cd66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running simulation for Workflow: Montage, Size: 10, Distribution: constant\n",
            "Total cost for Workflow Montage (constant, size=10): 1.8213205222212563\n",
            "Total time for Workflow Montage (constant, size=10): 73488.48\n",
            "Running simulation for Workflow: Montage, Size: 20, Distribution: constant\n",
            "Total cost for Workflow Montage (constant, size=20): 2.8452375154356586\n",
            "Total time for Workflow Montage (constant, size=20): 77930.88000000005\n",
            "Running simulation for Workflow: Montage, Size: 30, Distribution: constant\n",
            "Total cost for Workflow Montage (constant, size=30): 4.250921610724635\n",
            "Total time for Workflow Montage (constant, size=30): 87499.92000000001\n",
            "Running simulation for Workflow: Montage, Size: 40, Distribution: constant\n",
            "Total cost for Workflow Montage (constant, size=40): 6.372952891531288\n",
            "Total time for Workflow Montage (constant, size=40): 94500.36000000006\n",
            "Running simulation for Workflow: Montage, Size: 50, Distribution: constant\n",
            "Total cost for Workflow Montage (constant, size=50): 7.759845191188687\n",
            "Total time for Workflow Montage (constant, size=50): 89641.08000000012\n",
            "Running simulation for Workflow: Montage, Size: 60, Distribution: constant\n",
            "Total cost for Workflow Montage (constant, size=60): 9.022400692034026\n",
            "Total time for Workflow Montage (constant, size=60): 89128.20000000003\n",
            "Running simulation for Workflow: Montage, Size: 70, Distribution: constant\n",
            "Total cost for Workflow Montage (constant, size=70): 11.020953008597871\n",
            "Total time for Workflow Montage (constant, size=70): 90152.52000000003\n",
            "Running simulation for Workflow: Montage, Size: 80, Distribution: constant\n",
            "Total cost for Workflow Montage (constant, size=80): 12.411883780011467\n",
            "Total time for Workflow Montage (constant, size=80): 96847.08000000005\n",
            "Running simulation for Workflow: Montage, Size: 90, Distribution: constant\n",
            "Total cost for Workflow Montage (constant, size=90): 14.036195950325512\n",
            "Total time for Workflow Montage (constant, size=90): 98397.6000000001\n",
            "Running simulation for Workflow: Montage, Size: 100, Distribution: constant\n",
            "Total cost for Workflow Montage (constant, size=100): 15.011497477676917\n",
            "Total time for Workflow Montage (constant, size=100): 89327.04000000012\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 11):\n",
        "  run_simulation_with_results_tracking('Montage',10*i, 'constant', '/content/drive/My Drive/Zahra/Models/model_CyberShake_constant_10_cost.keras' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpvpbfmr2_9M"
      },
      "source": [
        "## Montage - uniform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j7GJQun3GTh",
        "outputId": "c22f2a7d-a919-4e64-87eb-b1d4f6a37c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running simulation for Workflow: Montage, Size: 10, Distribution: uniform\n",
            "numbers:  [3 3 4 2 6 4 1 2 1 4]\n",
            "Total cost for Workflow Montage (uniform, size=10): 1.3159717281574481\n",
            "Total time for Workflow Montage (uniform, size=10): 80777.40000000001\n",
            "Running simulation for Workflow: Montage, Size: 20, Distribution: uniform\n",
            "numbers:  [0 6 5 4 0 4 0 2 0 6 4 5 1 2 4 0 6 3 3 3]\n",
            "Total cost for Workflow Montage (uniform, size=20): 4.724837305318374\n",
            "Total time for Workflow Montage (uniform, size=20): 85131.00000000001\n",
            "Running simulation for Workflow: Montage, Size: 30, Distribution: uniform\n",
            "numbers:  [6 4 2 3 6 1 0 0 4 1 2 2 1 1 2 3 6 0 3 2 6 2 0 4 2 6 6 4 2 0]\n",
            "Total cost for Workflow Montage (uniform, size=30): 4.535389323983141\n",
            "Total time for Workflow Montage (uniform, size=30): 82063.08000000002\n",
            "Running simulation for Workflow: Montage, Size: 40, Distribution: uniform\n",
            "numbers:  [5 5 1 3 0 0 5 1 3 0 4 2 2 1 4 4 3 5 3 3 1 6 6 0 1 3 1 5 5 6 6 3 0 2 4 0 2\n",
            " 4 0 2]\n",
            "Total cost for Workflow Montage (uniform, size=40): 8.027756618668462\n",
            "Total time for Workflow Montage (uniform, size=40): 86212.32000000002\n",
            "Running simulation for Workflow: Montage, Size: 50, Distribution: uniform\n",
            "numbers:  [2 3 5 1 0 6 0 6 5 3 1 5 0 3 2 2 2 2 2 6 1 6 1 5 6 1 6 1 0 5 5 1 5 0 0 1 4\n",
            " 1 2 6 3 5 2 0 0 6 5 0 5 0]\n",
            "Total cost for Workflow Montage (uniform, size=50): 9.058020262890631\n",
            "Total time for Workflow Montage (uniform, size=50): 86006.64000000001\n",
            "Running simulation for Workflow: Montage, Size: 60, Distribution: uniform\n",
            "numbers:  [3 0 4 0 1 5 0 4 1 6 2 3 1 2 2 4 5 5 2 2 1 4 0 2 4 5 0 1 6 3 0 3 6 2 1 0 5\n",
            " 5 2 4 4 2 0 1 0 1 2 6 1 4 4 3 1 5 4 5 6 2 2 6]\n",
            "Total cost for Workflow Montage (uniform, size=60): 9.008139525620114\n",
            "Total time for Workflow Montage (uniform, size=60): 87905.16000000008\n",
            "Running simulation for Workflow: Montage, Size: 70, Distribution: uniform\n",
            "numbers:  [2 4 6 6 5 0 1 2 4 6 1 5 4 5 5 5 6 2 0 6 1 3 4 6 6 4 4 2 3 5 3 5 5 3 6 6 2\n",
            " 2 3 2 6 0 5 2 4 6 1 4 3 4 0 5 0 1 4 0 3 6 2 0 6 2 4 2 3 1 3 5 4 3]\n",
            "Total cost for Workflow Montage (uniform, size=70): 12.121205587489795\n",
            "Total time for Workflow Montage (uniform, size=70): 96822.83999999998\n",
            "Running simulation for Workflow: Montage, Size: 80, Distribution: uniform\n",
            "numbers:  [3 2 2 1 4 5 6 1 3 2 5 0 3 3 4 6 3 3 6 5 0 0 2 0 0 2 4 1 1 6 2 0 0 6 5 3 2\n",
            " 2 2 5 6 1 3 1 6 2 5 4 2 0 4 0 3 3 3 3 1 0 5 4 3 6 0 5 1 2 6 2 3 1 6 3 1 1\n",
            " 1 1 6 6 5 2]\n",
            "Total cost for Workflow Montage (uniform, size=80): 14.990842035952497\n",
            "Total time for Workflow Montage (uniform, size=80): 97920.59999999999\n",
            "Running simulation for Workflow: Montage, Size: 90, Distribution: uniform\n",
            "numbers:  [0 4 0 2 1 0 5 1 3 3 4 1 3 5 0 3 6 4 6 4 2 6 2 0 3 3 5 3 5 2 4 6 6 1 5 0 4\n",
            " 5 2 6 3 5 2 5 4 2 4 3 2 0 3 1 1 6 1 2 4 6 3 0 6 3 5 4 2 5 2 0 6 5 6 1 0 6\n",
            " 1 5 0 5 0 2 5 2 5 0 1 2 2 0 6 1]\n",
            "Total cost for Workflow Montage (uniform, size=90): 16.554285644167003\n",
            "Total time for Workflow Montage (uniform, size=90): 105641.04000000005\n",
            "Running simulation for Workflow: Montage, Size: 100, Distribution: uniform\n",
            "numbers:  [6 1 3 1 6 3 2 6 4 2 0 6 2 1 4 4 5 1 2 3 2 6 0 0 6 5 1 2 3 2 6 5 4 5 5 3 2\n",
            " 4 1 5 2 5 2 2 6 2 1 2 1 5 6 0 2 1 3 3 6 2 1 5 5 2 1 5 2 4 3 5 1 4 0 1 3 4\n",
            " 5 3 2 0 3 6 3 2 6 6 4 6 0 1 0 2 1 3 0 6 1 3 4 0 0 6]\n",
            "Total cost for Workflow Montage (uniform, size=100): 17.213087087963956\n",
            "Total time for Workflow Montage (uniform, size=100): 94202.87999999999\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 11):\n",
        "  run_simulation_with_results_tracking('Montage',10*i, 'uniform', '/content/drive/My Drive/Zahra/Models/model_CyberShake_constant_10_cost.keras' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWpstixI3WNq"
      },
      "source": [
        "## Sipht - constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AngfK6hh3bOO",
        "outputId": "cb0bfc0a-ae58-403a-cccb-0649283fbebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running simulation for Workflow: Sipht, Size: 10, Distribution: constant\n",
            "Total cost for Workflow Sipht (constant, size=10): 226.70940878525093\n",
            "Total time for Workflow Sipht (constant, size=10): 179475.72840000008\n",
            "Running simulation for Workflow: Sipht, Size: 20, Distribution: constant\n",
            "Total cost for Workflow Sipht (constant, size=20): 401.5558870449718\n",
            "Total time for Workflow Sipht (constant, size=20): 297655.1363999999\n",
            "Running simulation for Workflow: Sipht, Size: 30, Distribution: constant\n",
            "Total cost for Workflow Sipht (constant, size=30): 651.2580703948282\n",
            "Total time for Workflow Sipht (constant, size=30): 341018.2236000001\n",
            "Running simulation for Workflow: Sipht, Size: 40, Distribution: constant\n",
            "Total cost for Workflow Sipht (constant, size=40): 860.8435205895468\n",
            "Total time for Workflow Sipht (constant, size=40): 446655.96479999984\n",
            "Running simulation for Workflow: Sipht, Size: 50, Distribution: constant\n",
            "Total cost for Workflow Sipht (constant, size=50): 1026.7319620603955\n",
            "Total time for Workflow Sipht (constant, size=50): 547240.2095999996\n",
            "Running simulation for Workflow: Sipht, Size: 60, Distribution: constant\n",
            "Total cost for Workflow Sipht (constant, size=60): 1368.2811194126955\n",
            "Total time for Workflow Sipht (constant, size=60): 581542.8587999999\n",
            "Running simulation for Workflow: Sipht, Size: 70, Distribution: constant\n",
            "Total cost for Workflow Sipht (constant, size=70): 1480.4404070725311\n",
            "Total time for Workflow Sipht (constant, size=70): 803776.6884000005\n",
            "Running simulation for Workflow: Sipht, Size: 80, Distribution: constant\n",
            "Total cost for Workflow Sipht (constant, size=80): 1757.0124233890144\n",
            "Total time for Workflow Sipht (constant, size=80): 702336.8904000004\n",
            "Running simulation for Workflow: Sipht, Size: 90, Distribution: constant\n",
            "Total cost for Workflow Sipht (constant, size=90): 1976.6009858708633\n",
            "Total time for Workflow Sipht (constant, size=90): 792453.0300000001\n",
            "Running simulation for Workflow: Sipht, Size: 100, Distribution: constant\n",
            "Total cost for Workflow Sipht (constant, size=100): 2139.663249252487\n",
            "Total time for Workflow Sipht (constant, size=100): 817591.8816000002\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 11):\n",
        "  run_simulation_with_results_tracking('Sipht',10*i, 'constant', '/content/drive/My Drive/Zahra/Models/model_CyberShake_constant_10_cost.keras' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D81dg-wV3rJ3"
      },
      "source": [
        "## Sipht - uniform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77roJqmI3vhj",
        "outputId": "25fd85aa-af3a-420b-8762-fb771445af87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running simulation for Workflow: Sipht, Size: 10, Distribution: uniform\n",
            "numbers:  [2 0 1 0 3 0 0 2 4 3]\n",
            "Total cost for Workflow Sipht (uniform, size=10): 299.3915338264092\n",
            "Total time for Workflow Sipht (uniform, size=10): 192985.3536\n",
            "Running simulation for Workflow: Sipht, Size: 20, Distribution: uniform\n",
            "numbers:  [3 0 1 1 3 3 4 0 4 4 0 3 0 3 2 1 1 1 3 2]\n",
            "Total cost for Workflow Sipht (uniform, size=20): 1329.0248879305886\n",
            "Total time for Workflow Sipht (uniform, size=20): 589728.4632000002\n",
            "Running simulation for Workflow: Sipht, Size: 30, Distribution: uniform\n",
            "numbers:  [3 3 2 3 3 0 4 0 1 2 0 4 0 2 2 2 4 0 3 2 3 3 1 2 3 4 4 0 3 3]\n",
            "Total cost for Workflow Sipht (uniform, size=30): 864.9514620910531\n",
            "Total time for Workflow Sipht (uniform, size=30): 432270.4476000002\n",
            "Running simulation for Workflow: Sipht, Size: 40, Distribution: uniform\n",
            "numbers:  [3 4 0 2 4 4 3 0 0 0 1 2 2 3 4 1 2 4 1 1 3 4 2 4 2 0 4 0 3 2 0 4 0 2 3 2 1\n",
            " 1 1 0]\n",
            "Total cost for Workflow Sipht (uniform, size=40): 1876.8316443152935\n",
            "Total time for Workflow Sipht (uniform, size=40): 737154.3551999999\n",
            "Running simulation for Workflow: Sipht, Size: 50, Distribution: uniform\n",
            "numbers:  [3 4 1 2 0 0 3 1 0 0 1 4 4 2 3 3 3 3 1 2 4 2 1 3 4 0 2 3 2 2 0 1 4 0 0 3 2\n",
            " 1 1 4 2 3 1 1 1 3 3 1 0 4]\n",
            "Total cost for Workflow Sipht (uniform, size=50): 3133.452064476756\n",
            "Total time for Workflow Sipht (uniform, size=50): 1185408.581999999\n",
            "Running simulation for Workflow: Sipht, Size: 60, Distribution: uniform\n",
            "numbers:  [2 0 2 2 0 0 4 2 3 2 2 3 3 1 0 0 1 0 2 3 3 1 3 1 0 1 0 2 0 0 4 4 0 1 2 0 0\n",
            " 2 0 0 0 2 1 1 0 2 0 1 1 3 3 0 3 4 1 4 4 3 3 3]\n",
            "Total cost for Workflow Sipht (uniform, size=60): 2933.811055910213\n",
            "Total time for Workflow Sipht (uniform, size=60): 1067743.2564000005\n",
            "Running simulation for Workflow: Sipht, Size: 70, Distribution: uniform\n",
            "numbers:  [2 2 3 2 4 2 2 0 2 4 4 3 0 4 0 2 4 2 0 3 2 0 0 3 4 2 4 1 1 2 2 3 3 1 1 0 1\n",
            " 4 0 2 3 3 3 4 4 3 2 0 1 1 4 2 4 1 2 4 1 4 4 4 2 1 4 4 0 0 2 3 0 3]\n",
            "Total cost for Workflow Sipht (uniform, size=70): 2939.4644968191697\n",
            "Total time for Workflow Sipht (uniform, size=70): 1175765.2980000002\n",
            "Running simulation for Workflow: Sipht, Size: 80, Distribution: uniform\n",
            "numbers:  [0 1 1 3 3 3 1 1 2 2 0 4 1 3 2 2 4 3 1 1 0 1 4 0 3 4 0 1 1 4 3 2 3 2 0 2 1\n",
            " 4 4 2 0 2 1 2 0 3 3 2 3 0 1 0 4 4 0 3 0 3 2 2 2 0 2 4 2 0 3 2 4 4 0 3 3 4\n",
            " 0 0 4 0 1 2]\n",
            "Total cost for Workflow Sipht (uniform, size=80): 3867.5108419146895\n",
            "Total time for Workflow Sipht (uniform, size=80): 1418426.7131999996\n",
            "Running simulation for Workflow: Sipht, Size: 90, Distribution: uniform\n",
            "numbers:  [1 2 4 3 3 3 2 1 4 3 4 4 3 3 1 0 3 0 4 3 0 1 2 2 0 3 3 4 1 0 2 0 0 0 0 4 4\n",
            " 0 1 2 2 2 4 2 2 0 0 4 3 1 4 0 1 3 4 2 1 3 3 3 3 2 1 1 4 1 3 2 3 1 4 0 1 2\n",
            " 0 1 0 1 4 3 1 4 1 3 0 0 4 1 4 4]\n",
            "Total cost for Workflow Sipht (uniform, size=90): 5238.603358246811\n",
            "Total time for Workflow Sipht (uniform, size=90): 1934931.012\n",
            "Running simulation for Workflow: Sipht, Size: 100, Distribution: uniform\n",
            "numbers:  [1 4 3 0 1 0 2 4 1 4 1 0 3 3 4 0 2 0 4 2 0 1 1 3 1 0 3 0 2 3 1 3 4 1 3 3 2\n",
            " 1 2 3 1 3 4 2 4 0 1 4 0 1 1 0 0 2 4 4 4 4 4 4 0 2 1 3 4 1 2 0 0 0 2 2 2 1\n",
            " 2 4 0 1 3 3 2 2 1 0 2 1 4 3 1 4 2 1 3 3 4 3 3 2 1 0]\n",
            "Total cost for Workflow Sipht (uniform, size=100): 5933.271902265214\n",
            "Total time for Workflow Sipht (uniform, size=100): 2345738.5296000014\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 11):\n",
        "  run_simulation_with_results_tracking('Sipht',10*i, 'uniform', '/content/drive/My Drive/Zahra/Models/model_CyberShake_constant_10_cost.keras' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h-h3V5w4NXP"
      },
      "source": [
        "## Inspiral - constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr-AgY2X4Vh8",
        "outputId": "0e8b377d-ce7c-444c-b8c4-71d92ab3704d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running simulation for Workflow: Inspiral, Size: 10, Distribution: constant\n",
            "Total cost for Workflow Inspiral (constant, size=10): 3.829054480726243\n",
            "Total time for Workflow Inspiral (constant, size=10): 75765.60000000002\n",
            "Running simulation for Workflow: Inspiral, Size: 20, Distribution: constant\n",
            "Total cost for Workflow Inspiral (constant, size=20): 7.814274494044004\n",
            "Total time for Workflow Inspiral (constant, size=20): 77879.75999999998\n",
            "Running simulation for Workflow: Inspiral, Size: 30, Distribution: constant\n",
            "Total cost for Workflow Inspiral (constant, size=30): 11.927766726407391\n",
            "Total time for Workflow Inspiral (constant, size=30): 88037.28000000001\n",
            "Running simulation for Workflow: Inspiral, Size: 40, Distribution: constant\n",
            "Total cost for Workflow Inspiral (constant, size=40): 14.616347276799207\n",
            "Total time for Workflow Inspiral (constant, size=40): 92157.72000000003\n",
            "Running simulation for Workflow: Inspiral, Size: 50, Distribution: constant\n",
            "Total cost for Workflow Inspiral (constant, size=50): 19.013440265006878\n",
            "Total time for Workflow Inspiral (constant, size=50): 97960.20000000006\n",
            "Running simulation for Workflow: Inspiral, Size: 60, Distribution: constant\n",
            "Total cost for Workflow Inspiral (constant, size=60): 24.038174194363084\n",
            "Total time for Workflow Inspiral (constant, size=60): 96008.16000000002\n",
            "Running simulation for Workflow: Inspiral, Size: 70, Distribution: constant\n",
            "Total cost for Workflow Inspiral (constant, size=70): 27.0123261078337\n",
            "Total time for Workflow Inspiral (constant, size=70): 102504.12000000002\n",
            "Running simulation for Workflow: Inspiral, Size: 80, Distribution: constant\n",
            "Total cost for Workflow Inspiral (constant, size=80): 31.319783350377676\n",
            "Total time for Workflow Inspiral (constant, size=80): 107733.59999999995\n",
            "Running simulation for Workflow: Inspiral, Size: 90, Distribution: constant\n",
            "Total cost for Workflow Inspiral (constant, size=90): 34.97769121603269\n",
            "Total time for Workflow Inspiral (constant, size=90): 112465.08000000002\n",
            "Running simulation for Workflow: Inspiral, Size: 100, Distribution: constant\n",
            "Total cost for Workflow Inspiral (constant, size=100): 39.80768416202677\n",
            "Total time for Workflow Inspiral (constant, size=100): 111241.31999999995\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 11):\n",
        "  run_simulation_with_results_tracking('Inspiral',10*i, 'constant', '/content/drive/My Drive/Zahra/Models/model_CyberShake_constant_10_cost.keras' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHOfdObx4fIx"
      },
      "source": [
        "## Inspiral - uniform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70ubjjbz4kR7",
        "outputId": "41c56903-f716-4b27-8039-c68a9272ee23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running simulation for Workflow: Inspiral, Size: 10, Distribution: uniform\n",
            "numbers:  [1 2 2 0 1 3 1 0 2 0]\n",
            "Total cost for Workflow Inspiral (uniform, size=10): 12.836591317839257\n",
            "Total time for Workflow Inspiral (uniform, size=10): 101841.59999999998\n",
            "Running simulation for Workflow: Inspiral, Size: 20, Distribution: uniform\n",
            "numbers:  [1 2 0 0 0 3 3 1 3 3 1 2 1 3 1 2 2 2 0 2]\n",
            "Total cost for Workflow Inspiral (uniform, size=20): 19.466001345482585\n",
            "Total time for Workflow Inspiral (uniform, size=20): 95768.39999999998\n",
            "Running simulation for Workflow: Inspiral, Size: 30, Distribution: uniform\n",
            "numbers:  [3 0 1 3 2 1 3 1 1 1 1 3 0 3 1 0 3 1 0 1 1 1 2 3 0 3 2 3 2 0]\n",
            "Total cost for Workflow Inspiral (uniform, size=30): 28.107185456268\n",
            "Total time for Workflow Inspiral (uniform, size=30): 104433.36\n",
            "Running simulation for Workflow: Inspiral, Size: 40, Distribution: uniform\n",
            "numbers:  [0 2 0 2 0 2 0 1 2 2 1 1 2 1 0 3 1 0 1 3 0 3 1 2 0 0 2 0 3 2 2 1 1 0 3 0 0\n",
            " 1 2 2]\n",
            "Total cost for Workflow Inspiral (uniform, size=40): 57.49079266597649\n",
            "Total time for Workflow Inspiral (uniform, size=40): 116404.79999999996\n",
            "Running simulation for Workflow: Inspiral, Size: 50, Distribution: uniform\n",
            "numbers:  [3 0 1 0 2 1 0 0 3 0 3 0 3 1 1 3 2 3 1 1 2 3 1 2 1 0 1 3 3 3 2 3 0 2 2 0 2\n",
            " 0 0 2 2 0 1 3 2 3 1 3 0 3]\n",
            "Total cost for Workflow Inspiral (uniform, size=50): 56.32293648664318\n",
            "Total time for Workflow Inspiral (uniform, size=50): 127694.04000000002\n",
            "Running simulation for Workflow: Inspiral, Size: 60, Distribution: uniform\n",
            "numbers:  [1 0 1 2 1 3 1 3 1 0 0 3 2 3 3 2 0 3 3 2 3 0 2 3 3 0 3 3 1 3 2 0 3 0 1 0 2\n",
            " 0 2 2 3 1 3 2 2 2 1 3 0 1 0 2 1 1 2 2 0 0 0 0]\n",
            "Total cost for Workflow Inspiral (uniform, size=60): 70.92085301026529\n",
            "Total time for Workflow Inspiral (uniform, size=60): 130809.84000000004\n",
            "Running simulation for Workflow: Inspiral, Size: 70, Distribution: uniform\n",
            "numbers:  [0 3 3 0 1 3 3 0 2 1 1 1 0 3 1 3 0 1 2 0 2 1 1 0 0 3 0 2 1 1 2 0 0 3 1 1 3\n",
            " 0 1 2 3 2 2 0 1 0 2 1 0 3 0 3 1 3 0 2 2 1 2 0 0 1 0 0 2 2 3 3 3 3]\n",
            "Total cost for Workflow Inspiral (uniform, size=70): 93.05250050222399\n",
            "Total time for Workflow Inspiral (uniform, size=70): 149763.84000000008\n",
            "Running simulation for Workflow: Inspiral, Size: 80, Distribution: uniform\n",
            "numbers:  [0 3 2 1 1 3 0 3 2 0 1 2 3 3 1 1 0 1 2 0 3 2 2 1 2 1 2 0 1 2 1 1 3 2 3 2 2\n",
            " 1 1 1 2 2 2 2 1 1 2 1 1 2 0 2 0 1 0 2 3 2 3 3 0 3 2 3 0 1 3 3 3 3 1 0 0 3\n",
            " 2 2 2 2 0 0]\n",
            "Total cost for Workflow Inspiral (uniform, size=80): 74.69994316854371\n",
            "Total time for Workflow Inspiral (uniform, size=80): 124336.31999999993\n",
            "Running simulation for Workflow: Inspiral, Size: 90, Distribution: uniform\n",
            "numbers:  [1 1 0 2 2 2 2 3 2 3 1 1 0 0 1 2 1 3 1 2 3 0 2 3 3 1 0 3 2 2 1 1 1 1 2 3 0\n",
            " 0 3 0 1 3 3 0 1 0 0 2 2 1 3 2 3 0 0 1 1 1 2 1 1 2 1 2 3 1 0 3 1 0 2 1 3 0\n",
            " 3 2 2 1 3 3 3 3 1 0 2 1 0 0 1 1]\n",
            "Total cost for Workflow Inspiral (uniform, size=90): 88.85852817083078\n",
            "Total time for Workflow Inspiral (uniform, size=90): 135278.6399999999\n",
            "Running simulation for Workflow: Inspiral, Size: 100, Distribution: uniform\n",
            "numbers:  [0 0 3 1 2 0 3 2 0 3 2 2 3 0 2 2 3 0 3 2 2 1 2 0 3 3 1 2 3 0 0 1 3 0 2 2 3\n",
            " 0 1 0 1 2 0 2 3 3 0 0 3 0 1 1 1 3 3 2 0 3 1 1 1 1 1 1 2 2 0 1 2 3 2 1 3 2\n",
            " 2 3 2 3 0 3 3 2 1 2 1 1 2 1 0 2 2 2 1 1 2 3 1 0 0 0]\n",
            "Total cost for Workflow Inspiral (uniform, size=100): 108.19688711097382\n",
            "Total time for Workflow Inspiral (uniform, size=100): 156795.47999999992\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 11):\n",
        "  run_simulation_with_results_tracking('Inspiral',10*i, 'uniform', '/content/drive/My Drive/Zahra/Models/model_CyberShake_constant_10_cost.keras' )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}