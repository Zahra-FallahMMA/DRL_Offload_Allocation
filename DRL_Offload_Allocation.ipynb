{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizing Offloading and Resource Allocation in Fog Computing using Deep Q-Networks\n",
        "\n",
        "In the evolving landscape of Internet of Things (IoT) and edge computing, fog computing has emerged as a pivotal technology to complement cloud infrastructure by providing resources closer to the data source. This proximity aims to reduce latency, save bandwidth, and improve the overall efficiency of computational tasks. However, managing the offloading of tasks and allocating resources in a fog computing environment is a complex challenge due to the dynamic nature of IoT devices, the heterogeneity of resources, and varying network conditions.\n",
        "\n",
        "This project leverages Deep Q-Networks (DQN), a reinforcement learning technique, to optimize both offloading decisions and resource allocation in a fog computing environment. By employing DQN, the system learns to make intelligent decisions that balance computational cost and time efficiency, thereby enhancing the overall performance of the fog infrastructure.\n",
        "\n",
        "### Key Components and Methodology:\n",
        "\n",
        "1. **Fog Computing Environment**:\n",
        "   - Describes the architecture where fog nodes are placed between IoT devices and cloud data centers.\n",
        "   - Emphasizes the heterogeneous nature of fog nodes in terms of computational power, storage, and network connectivity.\n",
        "\n",
        "2. **Reinforcement Learning Framework**:\n",
        "   - Utilizes a DQN agent to learn the optimal policy for offloading and resource allocation.\n",
        "   - The agent interacts with the environment (fog nodes and IoT devices) to gather experiences, which are stored in a replay buffer.\n",
        "\n",
        "3. **Replay Buffer**:\n",
        "   - Used to store past experiences (state, action, reward, next state) to break the correlation between consecutive experiences and ensure stable learning.\n",
        "\n",
        "4. **Deep Q-Network Model**:\n",
        "   - A neural network model that approximates the Q-value function, which represents the expected cumulative reward of taking an action in a given state.\n",
        "   - The model is trained using the experiences sampled from the replay buffer.\n",
        "\n",
        "5. **Target Network**:\n",
        "   - A second neural network that provides stable target values, updated less frequently than the primary network, to stabilize training.\n",
        "\n",
        "6. **Training Process**:\n",
        "   - The agent iteratively interacts with the fog environment, making decisions on task offloading and resource allocation.\n",
        "   - At each step, it updates its knowledge based on the rewards received, aiming to improve the long-term efficiency in terms of cost and computation time.\n",
        "\n",
        "### Outcomes and Benefits:\n",
        "\n",
        "The application of DQN to fog computing environments facilitates the following improvements:\n",
        "\n",
        "- **Reduced Latency**: By strategically offloading tasks to the most appropriate fog nodes, the system minimizes latency, crucial for real-time applications.\n",
        "- **Cost Efficiency**: Intelligent resource allocation ensures optimal use of available resources, reducing operational costs.\n",
        "- **Scalability**: The reinforcement learning approach adapts to changes in the environment, making it scalable and robust to varying workloads and network conditions.\n",
        "- **Enhanced Performance**: The overall efficiency of the fog computing infrastructure is significantly improved, supporting a higher quality of service (QoS) for end-users.\n",
        "\n",
        "This project demonstrates the potential of reinforcement learning techniques like DQN in transforming fog computing environments, making them more adaptive, efficient, and capable of meeting the demanding needs of modern IoT applications.\n"
      ],
      "metadata": {
        "id": "aIHfTLBP7xxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q-Learning with Keras\n",
        "we will implement a Deep Q-Network (DQN) for a reinforcement learning task using Keras. The DQN will use a replay buffer to store experiences and a target network for stable learning."
      ],
      "metadata": {
        "id": "4CAWtBMk9NWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Required Libraries\n",
        "First, we import all the necessary libraries."
      ],
      "metadata": {
        "id": "tzeVaCBv9gCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "HFi0ZQcN9pjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Implement the Replay Buffer\n",
        "The replay buffer stores experiences and allows the agent to train on random batches of these experiences, which leads to more stable learning."
      ],
      "metadata": {
        "id": "_nY0HV0n90_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.discrete = discrete\n",
        "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        dtype = np.int8 if self.discrete else np.float32\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        if self.discrete:\n",
        "            actions = np.zeros(self.action_memory.shape[1])\n",
        "            actions[action] = 1.0\n",
        "            self.action_memory[index] = actions\n",
        "        else:\n",
        "            self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = 1 - done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size)\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "        return states, actions, rewards, states_, terminal\n"
      ],
      "metadata": {
        "id": "jGQLRhQ790Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: Implement the DQN Agent\n",
        "The agent will use two neural networks: one for the current Q-values and one for the target Q-values, which is updated less frequently for stability."
      ],
      "metadata": {
        "id": "O3-tIlYM-YsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, learning_rate=0.001, discount_factor=0.95, exploration_rate=1.0,\n",
        "                 exploration_decay=0.995, exploration_min=0.01, batch_size=64, memory_size=2000):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_min = exploration_min\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = ReplayBuffer(memory_size, state_size, action_size, discrete=True)\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() <= self.exploration_rate:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = np.array(state).reshape(1, -1)  # Ensure state is 2D\n",
        "        q_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        targets = self.model.predict(states, verbose=0)\n",
        "        target_next = self.target_model.predict(next_states, verbose=0)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            action_index = np.argmax(actions[i])  # Find the index of the action\n",
        "            if dones[i]:\n",
        "                targets[i, action_index] = rewards[i]\n",
        "            else:\n",
        "                targets[i, action_index] = rewards[i] + self.discount_factor * np.amax(target_next[i])\n",
        "\n",
        "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "        if self.exploration_rate > self.exploration_min:\n",
        "            self.exploration_rate *= self.exploration_decay\n",
        "\n",
        "        # Update target model every 10 episodes or steps\n",
        "        self.target_update_counter += 1\n",
        "        if self.target_update_counter % 10 == 0:\n",
        "            self.update_target_model()\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model = load_model(path)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        self.model.save(path)"
      ],
      "metadata": {
        "id": "KJc-r-qd-1EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow Parser\n",
        "This part of code code is designed to facilitate the representation and manipulation of workflows composed of tasks. Each task can have dependencies, and the goal is to model these workflows effectively for use in fog computing environments. It includes the ability to parse DAX files which describe these workflows, and to generate ensembles of workflows based on certain distributions.\n",
        "\n",
        "Main Components\n",
        "Task Class: Represents an individual task in the workflow.\n",
        "Workflow Class: Represents a collection of tasks forming a workflow.\n",
        "Parsing DAX Files: Functionality to parse XML-based DAX files to create workflow objects.\n",
        "Generating Workflow Ensembles: Creates multiple workflows based on specified distributions."
      ],
      "metadata": {
        "id": "EzoLxJw3OA0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Required Libraries"
      ],
      "metadata": {
        "id": "DQsa9VtnOgGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from io import StringIO\n",
        "import os\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jvtgUvaxOk-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Define the Task Class\n",
        "The Task class represents an individual computational task."
      ],
      "metadata": {
        "id": "mlk3oKWbOtpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Task:\n",
        "    def __init__(self, id, instructions):\n",
        "        self.id = id\n",
        "        self.instructions = instructions  # Execution time or computational instructions\n",
        "        self.children = []  # List of tasks that depend on this task\n",
        "        self.parents = []  # List of tasks this task depends on\n",
        "        self.executed = False  # Status of execution\n",
        "        self.executed_on = None  # Node this task was executed on\n",
        "        self.execution_time = 0  # Time taken to execute the task\n",
        "        self.cost = 0  # Cost of executing the task\n",
        "        self.comm_delay = 0  # Communication delay in seconds\n"
      ],
      "metadata": {
        "id": "QssdQVX-O77P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Define the Workflow Class\n",
        "The Workflow class manages a collection of Task objects and their dependencies."
      ],
      "metadata": {
        "id": "pQnJviusPCV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Workflow:\n",
        "    def __init__(self, id):\n",
        "        self.id = id  # Workflow identifier\n",
        "        self.tasks = {}  # Dictionary of tasks in the workflow\n",
        "\n",
        "    def add_task(self, task_id, instructions, parent_ids=[]):\n",
        "        if task_id not in self.tasks:\n",
        "            self.tasks[task_id] = Task(task_id, instructions)\n",
        "        task = self.tasks[task_id]\n",
        "        for parent_id in parent_ids:\n",
        "            if parent_id not in self.tasks:\n",
        "                self.tasks[parent_id] = Task(parent_id, 0)\n",
        "            parent_task = self.tasks[parent_id]\n",
        "            parent_task.children.append(task)\n",
        "            task.parents.append(parent_task)"
      ],
      "metadata": {
        "id": "8P_KDRzEPL0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Parse DAX File (Static Method)\n",
        "The parse_dax method parses a DAX XML file and constructs a Workflow object."
      ],
      "metadata": {
        "id": "PlEUb7Y0PUj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    @staticmethod\n",
        "    def parse_dax(file_path):\n",
        "        tree = ET.parse(file_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        workflow_id = root.attrib.get('name')\n",
        "        workflow = Workflow(workflow_id)\n",
        "\n",
        "        # Parse jobs\n",
        "        jobs = {job.attrib['id']: job for job in root.findall('{http://pegasus.isi.edu/schema/DAX}job')}\n",
        "\n",
        "        # Add jobs to workflow\n",
        "        for job_id, job in jobs.items():\n",
        "            instructions = float(job.attrib.get('runtime', 0))\n",
        "            workflow.add_task(job_id, instructions)\n",
        "\n",
        "        # Parse dependencies\n",
        "        for child in root.findall('{http://pegasus.isi.edu/schema/DAX}child'):\n",
        "            child_id = child.attrib['ref']\n",
        "            parent_ids = [parent.attrib['ref'] for parent in child.findall('{http://pegasus.isi.edu/schema/DAX}parent')]\n",
        "            workflow.add_task(child_id, 0, parent_ids)  # Adds a child node with its parent nodes, setting instructions to 0 to avoid overwrite\n",
        "\n",
        "        return workflow"
      ],
      "metadata": {
        "id": "ZXKL7eo0Pbbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Generate Ensemble of Workflows\n",
        "The ensemble_of_workflows method generates a list of workflows based on the specified distribution."
      ],
      "metadata": {
        "id": "7I9aqPGbPkoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    @staticmethod\n",
        "    def ensemble_of_workflows(name, size=100, distribution='constant', dax_path=''):\n",
        "        ensemble = []\n",
        "        directory_path = dax_path  # Directory containing DAX files\n",
        "\n",
        "        # List and filter files in directory\n",
        "        files = os.listdir(directory_path)\n",
        "        filtered_files = [file for file in files if name in file]\n",
        "\n",
        "        if distribution == 'constant':\n",
        "            pattern = r'100(?!\\d)'\n",
        "            for s in filtered_files:\n",
        "                if re.search(pattern, s):\n",
        "                    ensemble = [s] * size  # Replicate the matched file 'size' times\n",
        "                    break\n",
        "        else:\n",
        "            numbers = np.random.randint(0, len(filtered_files), size)\n",
        "            ensemble = [filtered_files[i] for i in numbers]  # Select random files based on uniform distribution\n",
        "\n",
        "        return ensemble"
      ],
      "metadata": {
        "id": "ZBwFmv6HPrrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dax files\n",
        "Importing Required Libraries:\n",
        "\n",
        "from google.colab import drive: This imports the drive module from the google.colab package. The module provides functions to interact with Google Drive, enabling you to mount your Google Drive storage within a Google Colab environment.\n",
        "import glob: This imports the glob module, which is used for finding all the pathnames matching a specified pattern according to the rules used by the Unix shell.\n",
        "Mounting Google Drive:\n",
        "\n",
        "drive.mount('/content/drive'): This mounts your Google Drive to the /content/drive directory within the Google Colab environment. After mounting, all files and folders stored in your Google Drive become accessible as if they are part of the local file system of the Colab environment. You’ll need to authorize this step, which usually involves a prompt to connect your Google account and grant the necessary permissions.\n",
        "Specifying the Folder Path:\n",
        "\n",
        "folder_path = '/content/drive/My Drive/Zahra/dax/': This assigns the directory path /content/drive/My Drive/Zahra/dax/ to the variable folder_path. This path points to a specific folder named dax located inside the Zahra directory in your Google Drive’s “My Drive” section. You can use this path to read files, write files, or perform other file operations within this folder."
      ],
      "metadata": {
        "id": "4eRFRuArV-00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import glob\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/My Drive/Zahra/dax/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "gCJgkeaTUFZx",
        "outputId": "d4224011-f381-4af0-d8c3-5f2c4c8f9ff6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-24def72d5113>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/Zahra/dax/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## simulation of a fog computing environment using reinforcement learning agents\n",
        "This code snippet implements a comprehensive simulation of a fog computing environment using reinforcement learning agents to optimize task offloading and resource allocation."
      ],
      "metadata": {
        "id": "M2f3GRdxXasT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step1 : Library Imports\n",
        "* random and numpy: Libraries for random number generation and numerical operations.\n",
        "* collections: Provides the deque for queue operations and defaultdict for easily creating default dictionary values.\n",
        "* itertools: Provides product for generating Cartesian products of input iterables, which is useful for hyperparameter tuning."
      ],
      "metadata": {
        "id": "6y9PG6ZWXvuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import deque, defaultdict\n",
        "from itertools import product\n",
        "\n",
        "# Set TensorFlow logging level to suppress detailed logs\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "6Xcls8BTX13t",
        "outputId": "5a9aaab3-aeed-46e4-d73a-ebd437dae177"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-abe8447e7ded>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TF_CPP_MIN_LOG_LEVEL'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step2 : Device Class\n",
        "This class represents a device in the simulation (IoT, Fog, or Server) with attributes such as ID, computational power (mips), and cost per hour. It also includes a task queue and methods to add and retrieve tasks from the queue."
      ],
      "metadata": {
        "id": "UG3W9dtOYzpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Device:\n",
        "    def __init__(self, id, mips, cost_per_hour):\n",
        "        self.id = id\n",
        "        self.mips = mips\n",
        "        self.cost_per_hour = cost_per_hour\n",
        "        self.queue = deque()\n",
        "\n",
        "    def add_task_to_queue(self, task):\n",
        "        self.queue.append(task)\n",
        "\n",
        "    def get_next_task(self):\n",
        "        return self.queue.popleft() if self.queue else None\n"
      ],
      "metadata": {
        "id": "3CdXIDPlZcsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step3 : Simulation Class\n",
        "Initializes the simulation parameters and creates instances of IoT, Fog, and Server devices. It also prepares the reinforcement learning agents and resets the simulation state."
      ],
      "metadata": {
        "id": "CtsvTaX4Zfar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Simulation:\n",
        "    def __init__(self, num_iot, num_fog, num_server, learning_rate=0.001, discount_factor=0.95,\n",
        "                 exploration_rate=1.0, exploration_decay=0.995, exploration_min=0.01, batch_size=64, memory_size=2000):\n",
        "        self.num_iot = num_iot\n",
        "        self.num_fog = num_fog\n",
        "        self.num_server = num_server\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_min = exploration_min\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_size = memory_size\n",
        "        self.reset()"
      ],
      "metadata": {
        "id": "E9PzgiSQZ1gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resets the simulation state, including creating new instances of devices, initializing total delay and cost, and setting up the reinforcement learning agents for IoT and broker tasks."
      ],
      "metadata": {
        "id": "RKcTevw8aDpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "     def reset(self):\n",
        "         self.iot_devices = [Device(f'iot_{i}', 500, 0) for i in range(self.num_iot)]\n",
        "         self.fog_devices = [Device(f'fog_{i}', 4000, 1) for i in range(self.num_fog)]\n",
        "         self.server_devices = [Device(f'server_{i}', 6000, 8) for i in range(self.num_server)]\n",
        "         self.total_delay = 0\n",
        "         self.total_cost = 0\n",
        "         self.workflows = []\n",
        "         self.ready_tasks = defaultdict(deque)\n",
        "         self.iot_agent = DQNAgent(state_size=3, action_size=2, learning_rate=self.learning_rate, discount_factor=self.discount_factor,\n",
        "                                   exploration_rate=self.exploration_rate, exploration_decay=self.exploration_decay,\n",
        "                                   exploration_min=self.exploration_min, batch_size=self.batch_size, memory_size=self.memory_size)\n",
        "         self.broker_agent = DQNAgent(state_size=4, action_size=2, learning_rate=self.learning_rate, discount_factor=self.discount_factor,\n",
        "                                      exploration_rate=self.exploration_rate, exploration_decay=self.exploration_decay,\n",
        "                                      exploration_min=self.exploration_min, batch_size=self.batch_size, memory_size=self.memory_size)\n"
      ],
      "metadata": {
        "id": "tDsCHPCXZ78a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step4 : Adding Workflows\n",
        "This method adds workflows to the simulation and assigns initial tasks to IoT devices."
      ],
      "metadata": {
        "id": "y9c62zaaaIOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def add_workflow(self, workflow):\n",
        "        self.workflows.extend(workflow)\n",
        "        for workflow in self.workflows:\n",
        "            iot_device = random.choice(self.iot_devices)\n",
        "            for task_id, task in workflow.tasks.items():\n",
        "                if not task.parents:\n",
        "                    iot_device.add_task_to_queue(task)\n"
      ],
      "metadata": {
        "id": "hU4GKcpWaU5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step5 : Task Execution\n",
        "Executes a given task on a specified device, adding the execution time and communication delay to the total delay and total cost."
      ],
      "metadata": {
        "id": "Mfi6NtKMaduG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def execute_task(self, task, device, comm_delay):\n",
        "        execution_time = task.instructions / device.mips / 1e6\n",
        "        task.execution_time = execution_time + comm_delay / 1000\n",
        "        task.comm_delay = comm_delay / 1000\n",
        "        if device.cost_per_hour == 0:\n",
        "            delay = task.execution_time\n",
        "            self.total_delay += delay\n",
        "            task.cost = 0\n",
        "        else:\n",
        "            cost = execution_time * device.cost_per_hour\n",
        "            self.total_cost += cost\n",
        "            task.cost = cost\n",
        "            self.total_delay += comm_delay / 1000\n",
        "        task.executed_on = device.id\n",
        "        task.executed = True\n"
      ],
      "metadata": {
        "id": "hD3KJkVgamB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step6 : Simulation Loop\n",
        "This is the core simulation loop, iterating over all devices and their task queues, and using the reinforcement learning agents to decide on task execution policies."
      ],
      "metadata": {
        "id": "CWp85ShoazoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def simulate(self):\n",
        "      while any([device.queue for device in self.iot_devices + self.fog_devices + self.server_devices]):\n",
        "         for device in self.iot_devices + self.fog_devices + self.server_devices:\n",
        "            if not device.queue:\n",
        "                continue\n",
        "\n",
        "            task = device.get_next_task()\n",
        "            if not task:\n",
        "                continue\n",
        "\n",
        "            pending_tasks = len(device.queue)\n",
        "            state = np.array([self.total_cost, self.total_delay, pending_tasks]).reshape(1, -1)\n",
        "\n",
        "            if device in self.iot_devices:\n",
        "                action = self.iot_agent.choose_action(state)\n",
        "            else:\n",
        "                broker_state = np.array([self.total_cost, self.total_delay, 0, pending_tasks]).reshape(1, -1)\n",
        "                action = self.broker_agent.choose_action(broker_state)\n",
        "\n",
        "            done = False\n",
        "\n",
        "          // More code snippets for task execution\n"
      ],
      "metadata": {
        "id": "1iewXM_wa3A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step7 : Hyperparameter Tuning\n",
        "This function performs hyperparameter tuning by running multiple simulations with different sets of parameters and selecting the best ones based on performance metrics."
      ],
      "metadata": {
        "id": "5XEU9S_rbF8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparameter_tuning(num_runs=100):\n",
        "    learning_rates = [0.0001]\n",
        "    discount_factors = [0.99]\n",
        "    exploration_rates = [0.5]\n",
        "    exploration_decays = [0.995]\n",
        "    exploration_mins = [ 0.05]\n",
        "\n",
        "    best_mean_delay = float('inf')\n",
        "    best_mean_cost = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    import os\n",
        "\n",
        "    for lr, df, er, ed, em in product(learning_rates, discount_factors, exploration_rates, exploration_decays, exploration_mins):\n",
        "            simulation = Simulation(num_iot=10, num_fog=8, num_server=5, learning_rate=lr, discount_factor=df, exploration_rate=er, exploration_decay=ed, exploration_min=em)\n",
        "            mean_delay, mean_cost = simulation.run_simulation(num_runs=num_runs, dax_path=folder_path)\n",
        "            #print(f\"Params: LR={lr}, DF={df}, ER={er}, ED={ed}, EM={em} -> Mean Delay: {mean_delay:.2f}, Mean Cost: ${mean_cost:.2f}\")\n",
        "            if mean_delay < best_mean_delay or (mean_delay == best_mean_delay and mean_cost < best_mean_cost):\n",
        "                best_mean_delay = mean_delay\n",
        "                best_mean_cost = mean_cost\n",
        "                best_params = (lr, df, er, ed, em)\n",
        "\n",
        "                simulation.iot_agent.save_model(f'best_iot_agent_model_.keras')\n",
        "                simulation.broker_agent.save_model(f'best_broker_agent_model.keras')\n",
        "\n",
        "\n",
        "            print(f\"Best Params: LR={best_params[0]}, DF={best_params[1]}, ER={best_params[2]}, ED={best_params[3]}, EM={best_params [4]} -> Mean Delay: {best_mean_delay:.2f}, Mean Cost: ${best_mean_cost:.2f}\")\n"
      ],
      "metadata": {
        "id": "j--cSsajbNYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run hyperparameter tuning\n",
        "hyperparameter_tuning(num_runs=1)"
      ],
      "metadata": {
        "id": "MRyNZguUbq1b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}