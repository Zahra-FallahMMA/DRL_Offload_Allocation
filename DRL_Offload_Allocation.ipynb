{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizing Offloading and Resource Allocation in Fog Computing using Deep Q-Networks\n",
        "\n",
        "In the evolving landscape of Internet of Things (IoT) and edge computing, fog computing has emerged as a pivotal technology to complement cloud infrastructure by providing resources closer to the data source. This proximity aims to reduce latency, save bandwidth, and improve the overall efficiency of computational tasks. However, managing the offloading of tasks and allocating resources in a fog computing environment is a complex challenge due to the dynamic nature of IoT devices, the heterogeneity of resources, and varying network conditions.\n",
        "\n",
        "This project leverages Deep Q-Networks (DQN), a reinforcement learning technique, to optimize both offloading decisions and resource allocation in a fog computing environment. By employing DQN, the system learns to make intelligent decisions that balance computational cost and time efficiency, thereby enhancing the overall performance of the fog infrastructure.\n",
        "\n",
        "### Key Components and Methodology:\n",
        "\n",
        "1. **Fog Computing Environment**:\n",
        "   - Describes the architecture where fog nodes are placed between IoT devices and cloud data centers.\n",
        "   - Emphasizes the heterogeneous nature of fog nodes in terms of computational power, storage, and network connectivity.\n",
        "\n",
        "2. **Reinforcement Learning Framework**:\n",
        "   - Utilizes a DQN agent to learn the optimal policy for offloading and resource allocation.\n",
        "   - The agent interacts with the environment (fog nodes and IoT devices) to gather experiences, which are stored in a replay buffer.\n",
        "\n",
        "3. **Replay Buffer**:\n",
        "   - Used to store past experiences (state, action, reward, next state) to break the correlation between consecutive experiences and ensure stable learning.\n",
        "\n",
        "4. **Deep Q-Network Model**:\n",
        "   - A neural network model that approximates the Q-value function, which represents the expected cumulative reward of taking an action in a given state.\n",
        "   - The model is trained using the experiences sampled from the replay buffer.\n",
        "\n",
        "5. **Target Network**:\n",
        "   - A second neural network that provides stable target values, updated less frequently than the primary network, to stabilize training.\n",
        "\n",
        "6. **Training Process**:\n",
        "   - The agent iteratively interacts with the fog environment, making decisions on task offloading and resource allocation.\n",
        "   - At each step, it updates its knowledge based on the rewards received, aiming to improve the long-term efficiency in terms of cost and computation time.\n",
        "\n",
        "### Outcomes and Benefits:\n",
        "\n",
        "The application of DQN to fog computing environments facilitates the following improvements:\n",
        "\n",
        "- **Reduced Latency**: By strategically offloading tasks to the most appropriate fog nodes, the system minimizes latency, crucial for real-time applications.\n",
        "- **Cost Efficiency**: Intelligent resource allocation ensures optimal use of available resources, reducing operational costs.\n",
        "- **Scalability**: The reinforcement learning approach adapts to changes in the environment, making it scalable and robust to varying workloads and network conditions.\n",
        "- **Enhanced Performance**: The overall efficiency of the fog computing infrastructure is significantly improved, supporting a higher quality of service (QoS) for end-users.\n",
        "\n",
        "This project demonstrates the potential of reinforcement learning techniques like DQN in transforming fog computing environments, making them more adaptive, efficient, and capable of meeting the demanding needs of modern IoT applications.\n"
      ],
      "metadata": {
        "id": "aIHfTLBP7xxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dual Deep Q-Learning\n",
        "we will implement a Deep Q-Network (DQN) for a reinforcement learning task using Keras. The DQN will use a replay buffer to store experiences and a target network for stable learning."
      ],
      "metadata": {
        "id": "4CAWtBMk9NWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://drive.google.com/uc?id=1iOfgegEl5e2B_iFysrUh4EFgoBqOhAEN\" alt=\"Image 1\" width=\"400\"/></td>\n",
        "    <td><img src=\"https://drive.google.com/uc?id=19cT0wbr37ZVGJxvKyeL37SCvpjvTt5yr\" alt=\"Image 2\" width=\"400\"/></td>\n",
        "   \n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "ll1ueo1HPcxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Required Libraries\n",
        "First, we import all the necessary libraries."
      ],
      "metadata": {
        "id": "tzeVaCBv9gCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "HFi0ZQcN9pjb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Implement the Replay Buffer\n",
        "Machin learning needs Independent and identically distributed random\n",
        "The replay buffer stores experiences and allows the agent to train on random batches of these experiences, which leads to IID.\n"
      ],
      "metadata": {
        "id": "_nY0HV0n90_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.discrete = discrete\n",
        "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        dtype = np.int8 if self.discrete else np.float32\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        if self.discrete:\n",
        "            actions = np.zeros(self.action_memory.shape[1])\n",
        "            actions[action] = 1.0\n",
        "            self.action_memory[index] = actions\n",
        "        else:\n",
        "            self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = 1 - done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size)\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "        return states, actions, rewards, states_, terminal\n"
      ],
      "metadata": {
        "id": "jGQLRhQ790Za"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: Implement the DQN Agent\n",
        "The agent will use two neural networks: one for the current Q-values and one for the target Q-values, which is updated less frequently for stability.\n",
        "\n",
        "* state_size: Input dimension of the state.\n",
        "* action_size: Number of possible actions.\n",
        "* learning_rate: Learning rate for the optimizer.\n",
        "* discount_factor (ℽ): Discount factor for future rewards.\n",
        "* exploration_rate (ϵ): Initial exploration rate (for ϵ-greedy policy).\n",
        "* exploration_decay: Decay rate for exploration to reduce ϵ over time.\n",
        "* exploration_min: Minimum exploration rate.\n",
        "* batch_size: Number of samples per training batch.\n",
        "* memory: Replay buffer to store experiences.\n",
        "* model: Q-network (online network).\n",
        "* target_model: Target Q-network (for stable Q-value computations).\n",
        "* target_update_counter: Counter to track when to update the target network.\n"
      ],
      "metadata": {
        "id": "O3-tIlYM-YsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, learning_rate=0.001, discount_factor=0.95, exploration_rate=1.0,\n",
        "                 exploration_decay=0.995, exploration_min=0.01, batch_size=64, memory_size=2000):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_min = exploration_min\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = ReplayBuffer(memory_size, state_size, action_size, discrete=True)\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() <= self.exploration_rate:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = np.array(state).reshape(1, -1)  # Ensure state is 2D\n",
        "        q_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        targets = self.model.predict(states, verbose=0)\n",
        "        target_next = self.target_model.predict(next_states, verbose=0)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            action_index = np.argmax(actions[i])  # Find the index of the action\n",
        "            if dones[i]:\n",
        "                targets[i, action_index] = rewards[i]\n",
        "            else:\n",
        "                targets[i, action_index] = rewards[i] + self.discount_factor * np.amax(target_next[i])\n",
        "\n",
        "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "        if self.exploration_rate > self.exploration_min:\n",
        "            self.exploration_rate *= self.exploration_decay\n",
        "\n",
        "        # Update target model every 10 episodes or steps\n",
        "        self.target_update_counter += 1\n",
        "        if self.target_update_counter % 10 == 0:\n",
        "            self.update_target_model()\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model = load_model(path)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        self.model.save(path)"
      ],
      "metadata": {
        "id": "KJc-r-qd-1EZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow Parser\n",
        "This part of code code is designed to facilitate the representation and manipulation of workflows composed of tasks. Each task can have dependencies, and the goal is to model these workflows effectively for use in fog computing environments. It includes the ability to parse DAX files which describe these workflows, and to generate ensembles of workflows based on certain distributions.\n",
        "\n",
        "Main Components\n",
        "Task Class: Represents an individual task in the workflow.\n",
        "Workflow Class: Represents a collection of tasks forming a workflow.\n",
        "Parsing DAX Files: Functionality to parse XML-based DAX files to create workflow objects.\n",
        "Generating Workflow Ensembles: Creates multiple workflows based on specified distributions."
      ],
      "metadata": {
        "id": "EzoLxJw3OA0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Required Libraries"
      ],
      "metadata": {
        "id": "DQsa9VtnOgGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from io import StringIO\n",
        "import os\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jvtgUvaxOk-j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Define the Task Class\n",
        "The Task class represents an individual computational task."
      ],
      "metadata": {
        "id": "mlk3oKWbOtpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Task:\n",
        "    def __init__(self, id, instructions, workflow_id):\n",
        "        self.id = id\n",
        "        self.instructions = instructions  # Execution time or computational instructions\n",
        "        self.children = []  # List of tasks that depend on this task\n",
        "        self.parents = []  # List of tasks this task depends on\n",
        "        self.executed = False  # Status of execution\n",
        "        self.executed_on = None  # Node this task was executed on\n",
        "        self.execution_time = 0  # Time taken to execute the task\n",
        "        self.cost = 0  # Cost of executing the task\n",
        "        self.comm_delay = 0  # Communication delay in seconds\n",
        "        self.workflow_id = workflow_id  # Workflow identifier to which this task belongs\n"
      ],
      "metadata": {
        "id": "QssdQVX-O77P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Define the Workflow Class\n",
        "The Workflow class manages a collection of Task objects and their dependencies."
      ],
      "metadata": {
        "id": "pQnJviusPCV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Workflow:\n",
        "    def __init__(self, id):\n",
        "        self.id = id  # Workflow identifier\n",
        "        self.tasks = {}  # Dictionary of tasks in the workflow\n",
        "\n",
        "    def add_task(self, task_id, instructions, parent_ids=[]):\n",
        "        if task_id not in self.tasks:\n",
        "            self.tasks[task_id] = Task(task_id, instructions, self.id)\n",
        "        task = self.tasks[task_id]\n",
        "        for parent_id in parent_ids:\n",
        "            if parent_id not in self.tasks:\n",
        "                self.tasks[parent_id] = Task(parent_id, 0, self.id)\n",
        "            parent_task = self.tasks[parent_id]\n",
        "            parent_task.children.append(task)\n",
        "            task.parents.append(parent_task)"
      ],
      "metadata": {
        "id": "8P_KDRzEPL0X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Parse DAX File (Static Method)\n",
        "The parse_dax method parses a DAX XML file and constructs a Workflow object."
      ],
      "metadata": {
        "id": "PlEUb7Y0PUj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_dax(file_path, workflow_id):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    workflow_id = workflow_id\n",
        "    workflow = Workflow(workflow_id)\n",
        "\n",
        "    # Parse jobs\n",
        "    jobs = {job.attrib['id']: job for job in root.findall('{http://pegasus.isi.edu/schema/DAX}job')}\n",
        "\n",
        "    # Add jobs to workflow\n",
        "    for job_id, job in jobs.items():\n",
        "        instructions = float(job.attrib.get('runtime', 0))\n",
        "        workflow.add_task(job_id, instructions)\n",
        "\n",
        "    # Parse dependencies\n",
        "    for child in root.findall('{http://pegasus.isi.edu/schema/DAX}child'):\n",
        "        child_id = child.attrib['ref']\n",
        "        parent_ids = [parent.attrib['ref'] for parent in child.findall('{http://pegasus.isi.edu/schema/DAX}parent')]\n",
        "        workflow.add_task(child_id, 0, parent_ids)  # Adds a child node with its parent nodes, setting instructions to 0 to avoid overwrite\n",
        "\n",
        "    return workflow\n",
        "\n",
        "Workflow.parse_dax = parse_dax"
      ],
      "metadata": {
        "id": "ZXKL7eo0Pbbi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Generate Ensemble of Workflows\n",
        "The ensemble_of_workflows method generates a list of workflows based on the specified distribution."
      ],
      "metadata": {
        "id": "7I9aqPGbPkoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_of_workflows(name, size=10, distribution='constant', dax_path=''):\n",
        "    ensemble = []\n",
        "    directory_path = dax_path  # Directory containing DAX files\n",
        "\n",
        "    # List and filter files in directory\n",
        "    files = os.listdir(directory_path)\n",
        "    filtered_files = [file for file in files if name in file]\n",
        "\n",
        "    if distribution == 'constant':\n",
        "        pattern = r'100(?!\\d)'\n",
        "        for s in filtered_files:\n",
        "            if re.search(pattern, s):\n",
        "                ensemble = [s] * size  # Replicate the matched file 'size' times\n",
        "                break\n",
        "    else:\n",
        "        numbers = np.random.randint(0, len(filtered_files), size)\n",
        "        ensemble = [filtered_files[i] for i in numbers]  # Select random files based on uniform distribution\n",
        "\n",
        "    return ensemble\n",
        "\n",
        "Workflow.ensemble_of_workflows = ensemble_of_workflows"
      ],
      "metadata": {
        "id": "ZBwFmv6HPrrF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dax files\n",
        "Importing Required Libraries:\n",
        "\n",
        "from google.colab import drive: This imports the drive module from the google.colab package. The module provides functions to interact with Google Drive, enabling you to mount your Google Drive storage within a Google Colab environment.\n",
        "import glob: This imports the glob module, which is used for finding all the pathnames matching a specified pattern according to the rules used by the Unix shell.\n",
        "Mounting Google Drive:\n",
        "\n",
        "drive.mount('/content/drive'): This mounts your Google Drive to the /content/drive directory within the Google Colab environment. After mounting, all files and folders stored in your Google Drive become accessible as if they are part of the local file system of the Colab environment. You’ll need to authorize this step, which usually involves a prompt to connect your Google account and grant the necessary permissions.\n",
        "Specifying the Folder Path:\n",
        "\n",
        "folder_path = '/content/drive/My Drive/Zahra/dax/': This assigns the directory path /content/drive/My Drive/Zahra/dax/ to the variable folder_path. This path points to a specific folder named dax located inside the Zahra directory in your Google Drive’s “My Drive” section. You can use this path to read files, write files, or perform other file operations within this folder."
      ],
      "metadata": {
        "id": "4eRFRuArV-00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import glob\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/My Drive/Zahra/dax/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCJgkeaTUFZx",
        "outputId": "9c5f07a8-2dfe-40e0-a9b4-a2fcd6e4c9a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## simulation of a fog computing environment using reinforcement learning agents\n",
        "This code snippet implements a comprehensive simulation of a fog computing environment using reinforcement learning agents to optimize task offloading and resource allocation."
      ],
      "metadata": {
        "id": "M2f3GRdxXasT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step1 : Library Imports\n",
        "* random and numpy: Libraries for random number generation and numerical operations.\n",
        "* collections: Provides the deque for queue operations and defaultdict for easily creating default dictionary values.\n",
        "* itertools: Provides product for generating Cartesian products of input iterables, which is useful for hyperparameter tuning."
      ],
      "metadata": {
        "id": "6y9PG6ZWXvuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import deque, defaultdict\n",
        "from itertools import product\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set TensorFlow logging level to suppress detailed logs\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "6Xcls8BTX13t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step2 : Device Class\n",
        "This class represents a device in the simulation (IoT, Fog, or Server) with attributes such as ID, computational power (mips), and cost per hour. It also includes a task queue and methods to add and retrieve tasks from the queue."
      ],
      "metadata": {
        "id": "UG3W9dtOYzpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Device:\n",
        "    def __init__(self, id, mips, cost_per_hour):\n",
        "        self.id = id\n",
        "        self.mips = mips\n",
        "        self.cost_per_hour = cost_per_hour\n",
        "        self.queue = deque()\n",
        "\n",
        "    def add_task_to_queue(self, task):\n",
        "        self.queue.append(task)\n",
        "\n",
        "    def get_next_task(self):\n",
        "        return self.queue.popleft() if self.queue else None\n"
      ],
      "metadata": {
        "id": "3CdXIDPlZcsf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step3 : Simulation Class\n",
        "Initializes the simulation parameters and creates instances of IoT, Fog, and Server devices. It also prepares the reinforcement learning agents and resets the simulation state."
      ],
      "metadata": {
        "id": "CtsvTaX4Zfar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Simulation:\n",
        "    def __init__(self, num_iot, num_fog, num_server, learning_rate=0.001, discount_factor=0.95,\n",
        "                 exploration_rate=1.0, exploration_decay=0.995, exploration_min=0.01, batch_size=64, memory_size=2000):\n",
        "        self.num_iot = num_iot\n",
        "        self.num_fog = num_fog\n",
        "        self.num_server = num_server\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_min = exploration_min\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_size = memory_size\n",
        "        self.reset()"
      ],
      "metadata": {
        "id": "E9PzgiSQZ1gZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Resets the simulation state, including creating new instances of devices, initializing total delay and cost, and setting up the reinforcement learning agents for IoT and broker tasks.\n",
        "* IoT devices’ processing power is 500 MIPS, and the processing power of fog and cloud servers are respectively 8 and 10 times faster than IoT devices.\n",
        "* We assumed fog and cloud servers are charging for leasing their resources with a price of 1 and 8 dollars in billing periods of 1 hour, respectively."
      ],
      "metadata": {
        "id": "RKcTevw8aDpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset(self):\n",
        "    self.iot_devices = [Device(f'iot_{i}', 500, 0) for i in range(self.num_iot)]\n",
        "    self.fog_devices = [Device(f'fog_{i}', 4000, 1) for i in range(self.num_fog)]\n",
        "    self.server_devices = [Device(f'server_{i}', 6000, 8) for i in range(self.num_server)]\n",
        "    self.total_delay = 0\n",
        "    self.total_cost = 0\n",
        "    self.workflows = []\n",
        "    self.ready_tasks = defaultdict(deque)\n",
        "    self.iot_agent = DQNAgent(state_size=3, action_size=2, learning_rate=self.learning_rate, discount_factor=self.discount_factor,\n",
        "                              exploration_rate=self.exploration_rate, exploration_decay=self.exploration_decay,\n",
        "                              exploration_min=self.exploration_min, batch_size=self.batch_size, memory_size=self.memory_size)\n",
        "    self.broker_agent = DQNAgent(state_size=4, action_size=2, learning_rate=self.learning_rate, discount_factor=self.discount_factor,\n",
        "                                exploration_rate=self.exploration_rate, exploration_decay=self.exploration_decay,\n",
        "                                exploration_min=self.exploration_min, batch_size=self.batch_size, memory_size=self.memory_size)\n",
        "\n",
        "Simulation.reset = reset"
      ],
      "metadata": {
        "id": "tDsCHPCXZ78a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step4 : Adding Workflows\n",
        "* This method adds workflows to the simulation and assigns initial tasks to IoT devices.\n",
        "* Each workflow enters to an IoT\n",
        "device, and local execution of this workflow’s tasks\n",
        "perform on that particular IoT device."
      ],
      "metadata": {
        "id": "y9c62zaaaIOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_workflow(self, workflow):\n",
        "  self.workflows.extend(workflow)\n",
        "  for workflow in self.workflows:\n",
        "      iot_device = random.choice(self.iot_devices)\n",
        "      for task_id, task in workflow.tasks.items():\n",
        "          if not task.parents:\n",
        "              iot_device.add_task_to_queue(task)\n",
        "\n",
        "Simulation.add_workflow = add_workflow"
      ],
      "metadata": {
        "id": "hU4GKcpWaU5q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step5 : Task Execution\n",
        "Executes a given task on a specified device, adding the execution time and communication delay to the total delay and total cost."
      ],
      "metadata": {
        "id": "Mfi6NtKMaduG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_task(self, task, device, comm_delay):\n",
        "    execution_time = task.instructions / device.mips / 1e6\n",
        "    task.execution_time = execution_time + comm_delay / 1000\n",
        "    task.comm_delay = comm_delay / 1000\n",
        "    if device.cost_per_hour == 0:\n",
        "        delay = task.execution_time\n",
        "        self.total_delay += delay\n",
        "        task.cost = 0\n",
        "    else:\n",
        "        cost = execution_time * device.cost_per_hour\n",
        "        self.total_cost += cost\n",
        "        task.cost = cost\n",
        "        self.total_delay += comm_delay / 1000\n",
        "    task.executed_on = device.id\n",
        "    task.executed = True\n",
        "\n",
        "Simulation.execute_task = execute_task"
      ],
      "metadata": {
        "id": "hD3KJkVgamB5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device_by_id(self, device_id):\n",
        "    for device in self.iot_devices + self.fog_devices + self.server_devices:\n",
        "        if device.id == device_id:\n",
        "            return device\n",
        "    return None\n",
        "\n",
        "def check_ready_tasks(self, task, device_id):\n",
        "    for child in task.children:\n",
        "        if all(parent.executed for parent in child.parents):\n",
        "            device = self.get_device_by_id(device_id)\n",
        "            if device:\n",
        "                device.add_task_to_queue(child)\n",
        "\n",
        "Simulation.get_device_by_id = get_device_by_id\n",
        "Simulation.check_ready_tasks = check_ready_tasks"
      ],
      "metadata": {
        "id": "LpwgeQWbtoeF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step6 : Simulation Loop\n",
        "This is the core simulation loop, iterating over all devices and their task queues, and using the reinforcement learning agents to decide on task execution policies."
      ],
      "metadata": {
        "id": "CWp85ShoazoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate(self):\n",
        "  while any([device.queue for device in self.iot_devices + self.fog_devices + self.server_devices]):\n",
        "    for device in self.iot_devices + self.fog_devices + self.server_devices:\n",
        "        if not device.queue:\n",
        "            continue\n",
        "\n",
        "        task = device.get_next_task()\n",
        "        if not task:\n",
        "            continue\n",
        "\n",
        "        pending_tasks = len(device.queue)\n",
        "        state = np.array([self.total_cost, self.total_delay, pending_tasks]).reshape(1, -1)\n",
        "\n",
        "        if device in self.iot_devices:\n",
        "            action = self.iot_agent.choose_action(state)\n",
        "        else:\n",
        "            broker_state = np.array([self.total_cost, self.total_delay, 0, pending_tasks]).reshape(1, -1)\n",
        "            action = self.broker_agent.choose_action(broker_state)\n",
        "\n",
        "        done = False\n",
        "\n",
        "        # Decide where to execute the task\n",
        "        if device in self.iot_devices and action == 0:\n",
        "            # Execute on IoT\n",
        "            self.execute_task(task, device, comm_delay=0)\n",
        "            reward = -task.execution_time\n",
        "            next_state = np.array([self.total_cost, self.total_delay, len(device.queue)]).reshape(1, -1)\n",
        "            if not device.queue:\n",
        "                done = True\n",
        "            self.iot_agent.remember(state, action, reward, next_state, done)\n",
        "            self.iot_agent.replay()\n",
        "            print(f\"task with id {task.id} from workflow {task.workflow_id} is done on iot device {device.id} - exe time = {task.execution_time} - cost = {task.cost}\")\n",
        "            print(f\"total cost = {self.total_cost} and total delay = {self.total_delay}\")\n",
        "        elif action == 0:  # Execute on Fog\n",
        "            broker_state = np.array([self.total_cost, self.total_delay, 0, pending_tasks]).reshape(1, -1)\n",
        "            fog_device = random.choice(self.fog_devices)\n",
        "            fog_device.add_task_to_queue(task)\n",
        "            broker_delay = 10 + 100  # Communication delay between IoT and Broker + Broker and Fog\n",
        "            self.execute_task(task, fog_device, comm_delay=broker_delay)\n",
        "            reward = -task.execution_time - task.cost\n",
        "            next_broker_state = np.array([self.total_cost, self.total_delay, broker_delay / 1000, pending_tasks]).reshape(1, -1)\n",
        "            if not fog_device.queue:\n",
        "                done = True\n",
        "            self.broker_agent.remember(broker_state, action, reward, next_broker_state, done)\n",
        "            self.broker_agent.replay()\n",
        "            print(f\"task with id {task.id} from workflow {task.workflow_id} is done on iot device {fog_device.id} - exe time = {task.execution_time} - cost = {task.cost}\")\n",
        "            print(f\"total cost = {self.total_cost} and total delay = {self.total_delay}\")\n",
        "        else:  # Execute on Server\n",
        "            broker_state = np.array([self.total_cost, self.total_delay, 0, pending_tasks]).reshape(1, -1)\n",
        "            server_device = random.choice(self.server_devices)\n",
        "            server_device.add_task_to_queue(task)\n",
        "            broker_delay = 10 + 300  # Communication delay between IoT and Broker + Broker and Server\n",
        "            self.execute_task(task, server_device, comm_delay=broker_delay)\n",
        "            reward = -task.execution_time - task.cost\n",
        "            next_broker_state = np.array([self.total_cost, self.total_delay, broker_delay / 1000, pending_tasks]).reshape(1, -1)\n",
        "            if not server_device.queue:\n",
        "                done = True\n",
        "            self.broker_agent.remember(broker_state, action, reward, next_broker_state, done)\n",
        "            self.broker_agent.replay()\n",
        "            print(f\"task with id {task.id} from workflow {task.workflow_id} is done on iot device {device.id} - exe time = {task.execution_time} - cost = {task.cost}\")\n",
        "            print(f\"total cost = {self.total_cost} and total delay = {self.total_delay}\")\n",
        "\n",
        "        self.check_ready_tasks(task, device.id)\n",
        "        next_state = np.array([self.total_cost, self.total_delay, len(device.queue)]).reshape(1, -1)\n",
        "        if not device.queue:\n",
        "            done = True\n",
        "        self.iot_agent.remember(state, action, reward, next_state, done)\n",
        "        self.iot_agent.replay()\n",
        "\n",
        "\n",
        "\n",
        "Simulation.simulate = simulate\n"
      ],
      "metadata": {
        "id": "1iewXM_wa3A-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models(self, iot_model_path, broker_model_path):\n",
        "    self.iot_agent.load_model(iot_model_path)\n",
        "    self.broker_agent.load_model(broker_model_path)\n",
        "\n",
        "\n",
        "def run_simulation(self, num_runs=1, dax_path='', load_models=False, iot_model_path='', broker_model_path=''):\n",
        "    total_delays = []\n",
        "    total_costs = []\n",
        "    ws = []\n",
        "    # Parse workflows from DAX file\n",
        "    w_id = 0\n",
        "    names_of_workflows_in_ensemble = Workflow.ensemble_of_workflows(name = 'CyberShake', size=100, distribution = 'constant', dax_path = dax_path)\n",
        "    for workflow_name in names_of_workflows_in_ensemble:\n",
        "      ws.append(Workflow.parse_dax(dax_path + workflow_name, w_id ))\n",
        "      w_id +=1\n",
        "\n",
        "\n",
        "    if load_models:\n",
        "      self.load_models(iot_model_path, broker_model_path)\n",
        "\n",
        "    for _ in range(num_runs):\n",
        "        self.reset()\n",
        "        # Add parsed workflow to simulation\n",
        "        self.add_workflow(ws)\n",
        "\n",
        "        # Run simulation\n",
        "        self.simulate()\n",
        "\n",
        "        # Collect results\n",
        "        total_delays.append(self.total_delay)\n",
        "        total_costs.append(self.total_cost)\n",
        "        print(self.total_delay)\n",
        "\n",
        "    mean_delay = np.mean(total_delays)\n",
        "    mean_cost = np.mean(total_costs)\n",
        "    return mean_delay, mean_cost\n",
        "\n",
        "Simulation.run_simulation = run_simulation"
      ],
      "metadata": {
        "id": "gVFGd6pJ24GJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step7 : Hyperparameter Tuning\n",
        "This function performs hyperparameter tuning by running multiple simulations with different sets of parameters and selecting the best ones based on performance metrics.\n",
        "* A simulation environment was implemented using\n",
        "PYTHON to study and analyze the proposed method,\n",
        "including 10 IoT devices, 8 fog servers, and 5\n",
        "cloud servers.\n",
        "* We have considered homogeneous resources with the same processing power in\n",
        "each tier; also, one VM is assumed for each server.\n",
        "Because of the limitation in real benchmarks for fog\n",
        "environment , we have used configurations and\n",
        "parameters of previous studies."
      ],
      "metadata": {
        "id": "5XEU9S_rbF8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparameter_tuning(num_runs=100):\n",
        "    learning_rates = [0.0001]\n",
        "    discount_factors = [0.99]\n",
        "    exploration_rates = [0.5]\n",
        "    exploration_decays = [0.995]\n",
        "    exploration_mins = [ 0.05]\n",
        "\n",
        "    best_mean_delay = float('inf')\n",
        "    best_mean_cost = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    import os\n",
        "\n",
        "    for lr, df, er, ed, em in product(learning_rates, discount_factors, exploration_rates, exploration_decays, exploration_mins):\n",
        "            simulation = Simulation(num_iot=10, num_fog=8, num_server=5, learning_rate=lr, discount_factor=df, exploration_rate=er, exploration_decay=ed, exploration_min=em)\n",
        "            mean_delay, mean_cost = simulation.run_simulation(num_runs=num_runs, dax_path=folder_path)\n",
        "            #print(f\"Params: LR={lr}, DF={df}, ER={er}, ED={ed}, EM={em} -> Mean Delay: {mean_delay:.2f}, Mean Cost: ${mean_cost:.2f}\")\n",
        "            if mean_delay < best_mean_delay or (mean_delay == best_mean_delay and mean_cost < best_mean_cost):\n",
        "                best_mean_delay = mean_delay\n",
        "                best_mean_cost = mean_cost\n",
        "                best_params = (lr, df, er, ed, em)\n",
        "\n",
        "                #simulation.iot_agent.save_model(f'best_iot_agent_model_.keras')\n",
        "                #simulation.broker_agent.save_model(f'best_broker_agent_model.keras')\n",
        "\n",
        "\n",
        "            print(f\"Best Params: LR={best_params[0]}, DF={best_params[1]}, ER={best_params[2]}, ED={best_params[3]}, EM={best_params [4]} -> Mean Delay: {best_mean_delay:.2f}, Mean Cost: ${best_mean_cost:.2f}\")\n"
      ],
      "metadata": {
        "id": "j--cSsajbNYU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run hyperparameter tuning\n",
        "hyperparameter_tuning(num_runs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MRyNZguUbq1b",
        "outputId": "7f857157-d52e-4396-955d-9640ef6adfec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "task with id ID00002 from workflow 17 is done on iot device iot_0 - exe time = 3.0889999999999997e-07 - cost = 0\n",
            "total cost = 0 and total delay = 3.0889999999999997e-07\n",
            "task with id ID00002 from workflow 8 is done on iot device iot_1 - exe time = 3.0889999999999997e-07 - cost = 0\n",
            "total cost = 0 and total delay = 6.177999999999999e-07\n",
            "task with id ID00002 from workflow 2 is done on iot device iot_2 - exe time = 3.0889999999999997e-07 - cost = 0\n",
            "total cost = 0 and total delay = 9.267e-07\n",
            "task with id ID00002 from workflow 24 is done on iot device iot_3 - exe time = 3.0889999999999997e-07 - cost = 0\n",
            "total cost = 0 and total delay = 1.2355999999999999e-06\n",
            "task with id ID00002 from workflow 1 is done on iot device iot_4 - exe time = 3.0889999999999997e-07 - cost = 0\n",
            "total cost = 0 and total delay = 1.5444999999999998e-06\n",
            "task with id ID00002 from workflow 0 is done on iot device iot_5 - exe time = 3.0889999999999997e-07 - cost = 0\n",
            "total cost = 0 and total delay = 1.8533999999999997e-06\n",
            "task with id ID00002 from workflow 7 is done on iot device iot_6 - exe time = 3.0889999999999997e-07 - cost = 0\n",
            "total cost = 0 and total delay = 2.1622999999999996e-06\n",
            "task with id ID00002 from workflow 3 is done on iot device iot_7 - exe time = 3.0889999999999997e-07 - cost = 0\n",
            "total cost = 0 and total delay = 2.4711999999999997e-06\n",
            "task with id ID00002 from workflow 14 is done on iot device iot_8 - exe time = 0.3100000257416667 - cost = 2.0593333333333334e-07\n",
            "total cost = 2.0593333333333334e-07 and total delay = 0.3100024712\n",
            "task with id ID00002 from workflow 5 is done on iot device iot_9 - exe time = 3.0889999999999997e-07 - cost = 0\n",
            "total cost = 2.0593333333333334e-07 and total delay = 0.31000278010000004\n",
            "task with id ID00002 from workflow 14 is done on iot device fog_3 - exe time = 0.1100000386125 - cost = 3.8612499999999996e-08\n",
            "total cost = 2.4454583333333333e-07 and total delay = 0.42000278010000003\n",
            "task with id ID00011 from workflow 17 is done on iot device iot_0 - exe time = 2.4630000000000003e-07 - cost = 0\n",
            "total cost = 2.4454583333333333e-07 and total delay = 0.42000302640000003\n",
            "task with id ID00011 from workflow 8 is done on iot device iot_1 - exe time = 2.4630000000000003e-07 - cost = 0\n",
            "total cost = 2.4454583333333333e-07 and total delay = 0.42000327270000004\n",
            "task with id ID00011 from workflow 2 is done on iot device iot_2 - exe time = 0.310000020525 - cost = 1.642e-07\n",
            "total cost = 4.0874583333333333e-07 and total delay = 0.7300032727000001\n",
            "task with id ID00011 from workflow 24 is done on iot device iot_3 - exe time = 2.4630000000000003e-07 - cost = 0\n",
            "total cost = 4.0874583333333333e-07 and total delay = 0.730003519\n",
            "task with id ID00011 from workflow 1 is done on iot device iot_4 - exe time = 2.4630000000000003e-07 - cost = 0\n",
            "total cost = 4.0874583333333333e-07 and total delay = 0.7300037653\n",
            "task with id ID00011 from workflow 0 is done on iot device iot_5 - exe time = 2.4630000000000003e-07 - cost = 0\n",
            "total cost = 4.0874583333333333e-07 and total delay = 0.7300040116\n",
            "task with id ID00011 from workflow 7 is done on iot device iot_6 - exe time = 0.310000020525 - cost = 1.642e-07\n",
            "total cost = 5.729458333333333e-07 and total delay = 1.0400040116\n",
            "task with id ID00011 from workflow 3 is done on iot device iot_7 - exe time = 0.310000020525 - cost = 1.642e-07\n",
            "total cost = 7.371458333333333e-07 and total delay = 1.3500040116\n",
            "task with id ID00011 from workflow 14 is done on iot device iot_8 - exe time = 2.4630000000000003e-07 - cost = 0\n",
            "total cost = 7.371458333333333e-07 and total delay = 1.3500042579\n",
            "task with id ID00011 from workflow 5 is done on iot device iot_9 - exe time = 2.4630000000000003e-07 - cost = 0\n",
            "total cost = 7.371458333333333e-07 and total delay = 1.3500045042\n",
            "task with id ID00002 from workflow 14 is done on iot device fog_3 - exe time = 0.1100000386125 - cost = 3.8612499999999996e-08\n",
            "total cost = 7.757583333333333e-07 and total delay = 1.4600045042\n",
            "task with id ID00011 from workflow 7 is done on iot device fog_1 - exe time = 0.1100000307875 - cost = 3.0787500000000004e-08\n",
            "total cost = 8.065458333333333e-07 and total delay = 1.5700045042000002\n",
            "task with id ID00003 from workflow 14 is done on iot device fog_4 - exe time = 0.1100000137225 - cost = 1.3722500000000001e-08\n",
            "total cost = 8.202683333333332e-07 and total delay = 1.6800045042000002\n",
            "task with id ID00022 from workflow 17 is done on iot device iot_0 - exe time = 2.2040000000000001e-07 - cost = 0\n",
            "total cost = 8.202683333333332e-07 and total delay = 1.6800047246000003\n",
            "task with id ID00022 from workflow 8 is done on iot device iot_1 - exe time = 2.2040000000000001e-07 - cost = 0\n",
            "total cost = 8.202683333333332e-07 and total delay = 1.6800049450000003\n",
            "task with id ID00022 from workflow 2 is done on iot device iot_2 - exe time = 0.3100000183666667 - cost = 1.4693333333333333e-07\n",
            "total cost = 9.672016666666665e-07 and total delay = 1.9900049450000004\n",
            "task with id ID00022 from workflow 24 is done on iot device iot_3 - exe time = 2.2040000000000001e-07 - cost = 0\n",
            "total cost = 9.672016666666665e-07 and total delay = 1.9900051654000004\n",
            "task with id ID00022 from workflow 1 is done on iot device iot_4 - exe time = 2.2040000000000001e-07 - cost = 0\n",
            "total cost = 9.672016666666665e-07 and total delay = 1.9900053858000004\n",
            "task with id ID00022 from workflow 0 is done on iot device iot_5 - exe time = 2.2040000000000001e-07 - cost = 0\n",
            "total cost = 9.672016666666665e-07 and total delay = 1.9900056062000004\n",
            "task with id ID00022 from workflow 7 is done on iot device iot_6 - exe time = 2.2040000000000001e-07 - cost = 0\n",
            "total cost = 9.672016666666665e-07 and total delay = 1.9900058266000005\n",
            "task with id ID00022 from workflow 3 is done on iot device iot_7 - exe time = 0.3100000183666667 - cost = 1.4693333333333333e-07\n",
            "total cost = 1.114135e-06 and total delay = 2.3000058266000005\n",
            "task with id ID00022 from workflow 14 is done on iot device iot_8 - exe time = 2.2040000000000001e-07 - cost = 0\n",
            "total cost = 1.114135e-06 and total delay = 2.3000060470000006\n",
            "task with id ID00022 from workflow 5 is done on iot device iot_9 - exe time = 2.2040000000000001e-07 - cost = 0\n",
            "total cost = 1.114135e-06 and total delay = 2.3000062674000006\n",
            "task with id ID00011 from workflow 7 is done on iot device fog_1 - exe time = 0.310000020525 - cost = 1.642e-07\n",
            "total cost = 1.278335e-06 and total delay = 2.6100062674000006\n",
            "task with id ID00002 from workflow 14 is done on iot device fog_3 - exe time = 0.3100000257416667 - cost = 2.0593333333333334e-07\n",
            "total cost = 1.4842683333333334e-06 and total delay = 2.9200062674000007\n",
            "task with id ID00003 from workflow 14 is done on iot device fog_6 - exe time = 0.1100000137225 - cost = 1.3722500000000001e-08\n",
            "total cost = 1.4979908333333335e-06 and total delay = 3.0300062674000006\n",
            "task with id ID00003 from workflow 14 is done on iot device fog_6 - exe time = 0.3100000091483333 - cost = 7.318666666666666e-08\n",
            "total cost = 1.5711775000000002e-06 and total delay = 3.3400062674000006\n",
            "task with id ID00011 from workflow 3 is done on iot device server_1 - exe time = 0.310000020525 - cost = 1.642e-07\n",
            "total cost = 1.7353775000000003e-06 and total delay = 3.6500062674000007\n",
            "task with id ID00003 from workflow 14 is done on iot device fog_3 - exe time = 0.1100000137225 - cost = 1.3722500000000001e-08\n",
            "total cost = 1.7491000000000003e-06 and total delay = 3.7600062674000005\n",
            "task with id ID00022 from workflow 2 is done on iot device fog_1 - exe time = 0.11000002755 - cost = 2.7550000000000002e-08\n",
            "total cost = 1.7766500000000004e-06 and total delay = 3.8700062674000004\n",
            "task with id ID00005 from workflow 14 is done on iot device fog_3 - exe time = 0.110000013695 - cost = 1.3695000000000001e-08\n",
            "total cost = 1.7903450000000004e-06 and total delay = 3.9800062674000003\n",
            "task with id ID00033 from workflow 17 is done on iot device iot_0 - exe time = 1.6791999999999998e-07 - cost = 0\n",
            "total cost = 1.7903450000000004e-06 and total delay = 3.9800064353200004\n",
            "task with id ID00033 from workflow 8 is done on iot device iot_1 - exe time = 1.6791999999999998e-07 - cost = 0\n",
            "total cost = 1.7903450000000004e-06 and total delay = 3.9800066032400006\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5b370088bb97>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run hyperparameter tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhyperparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-54418d45d360>\u001b[0m in \u001b[0;36mhyperparameter_tuning\u001b[0;34m(num_runs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_decays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0msimulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_fog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0med\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexploration_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mmean_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdax_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;31m#print(f\"Params: LR={lr}, DF={df}, ER={er}, ED={ed}, EM={em} -> Mean Delay: {mean_delay:.2f}, Mean Cost: ${mean_cost:.2f}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmean_delay\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_mean_delay\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmean_delay\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbest_mean_delay\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmean_cost\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_mean_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-87d80d0a122d>\u001b[0m in \u001b[0;36mrun_simulation\u001b[0;34m(self, num_runs, dax_path, load_models, iot_model_path, broker_model_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Run simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Collect results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-2fcbf1a7fa9b>\u001b[0m in \u001b[0;36msimulate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miot_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miot_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"task with id {task.id} from workflow {task.workflow_id} is done on iot device {device.id} - exe time = {task.execution_time} - cost = {task.cost}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"total cost = {self.total_cost} and total delay = {self.total_delay}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2dbe3a58ee61>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Create an iterator that yields batches for one epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         epoch_iterator = TFEpochIterator(\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribute_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2354\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2355\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2357\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\u001b[0m in \u001b[0;36m_flat_map\u001b[0;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_flat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-private-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_FlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     variant_tensor = gen_dataset_ops.flat_map_dataset(\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mflat_map_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, metadata, name)\u001b[0m\n\u001b[1;32m   2424\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2426\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   2427\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FlatMapDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m         \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}