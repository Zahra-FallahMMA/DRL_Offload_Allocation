{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMuz/9Fhjt0VMHVhk9Po0Ys",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zahra-FallahMMA/DRL_Offload_Allocation/blob/main/DRL_Offload_Allocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import Libraries"
      ],
      "metadata": {
        "id": "PcuUUZU1G5k4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIP4GlsbGwAx",
        "outputId": "a573e320-a2b9-4b30-8eed-d303a10c29fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simpy\n",
            "  Downloading simpy-4.1.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "Downloading simpy-4.1.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: simpy\n",
            "Successfully installed simpy-4.1.1\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from io import StringIO\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import deque, defaultdict\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from itertools import product\n",
        "import tensorflow as tf\n",
        "!pip install simpy\n",
        "import simpy\n",
        "\n",
        "# Set TensorFlow logging level to suppress detailed logs\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class ReplayBuffer"
      ],
      "metadata": {
        "id": "n3fKTuuQHDo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.discrete = discrete\n",
        "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        dtype = np.int8 if self.discrete else np.float32\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        if self.discrete:\n",
        "            actions = np.zeros(self.action_memory.shape[1])\n",
        "            actions[action] = 1.0\n",
        "            self.action_memory[index] = actions\n",
        "        else:\n",
        "            self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = 1 - done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size)\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "        return states, actions, rewards, states_, terminal\n"
      ],
      "metadata": {
        "id": "SRdAlbgAG3w8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class DQNAgent"
      ],
      "metadata": {
        "id": "lCfIwrJDHK33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, learning_rate=0.001, discount_factor=0.95, exploration_rate=1.0,\n",
        "                 exploration_decay=0.995, exploration_min=0.01, batch_size=64, memory_size=2000):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_min = exploration_min\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = ReplayBuffer(memory_size, state_size, action_size, discrete=True)\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() <= self.exploration_rate:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = np.array(state).reshape(1, -1)  # Ensure state is 2D\n",
        "        q_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        targets = self.model.predict(states, verbose=0)\n",
        "        target_next = self.target_model.predict(next_states, verbose=0)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            action_index = np.argmax(actions[i])  # Find the index of the action\n",
        "            if dones[i]:\n",
        "                targets[i, action_index] = rewards[i]\n",
        "            else:\n",
        "                targets[i, action_index] = rewards[i] + self.discount_factor * np.amax(target_next[i])\n",
        "\n",
        "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "        if self.exploration_rate > self.exploration_min:\n",
        "            self.exploration_rate *= self.exploration_decay\n",
        "\n",
        "        # Update target model every 10 episodes or steps\n",
        "        self.target_update_counter += 1\n",
        "        if self.target_update_counter % 10 == 0:\n",
        "            self.update_target_model()\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.model = load_model(path)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        self.model.save(path)"
      ],
      "metadata": {
        "id": "qreTNfv_HP9X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class Task"
      ],
      "metadata": {
        "id": "sc5cwMB6HUAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Task:\n",
        "    def __init__(self, id, instructions, workflow_id):\n",
        "        self.id = id\n",
        "        self.instructions = instructions  # Execution time or computational instructions\n",
        "        self.children = []  # List of tasks that depend on this task\n",
        "        self.parents = []  # List of tasks this task depends on\n",
        "        self.executed = False  # Status of execution\n",
        "        self.executed_on = None  # Node this task was executed on\n",
        "        self.execution_time = 0  # Time taken to execute the task\n",
        "        self.cost = 0  # Cost of executing the task\n",
        "        self.comm_delay = 0  # Communication delay in seconds\n",
        "        self.workflow_id = workflow_id  # Workflow identifier to which this task belongs\n"
      ],
      "metadata": {
        "id": "dWqg63i_HW5y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Workflow"
      ],
      "metadata": {
        "id": "V6GeROhDHben"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Workflow:\n",
        "    def __init__(self, id):\n",
        "        self.id = id  # Workflow identifier\n",
        "        self.tasks = {}  # Dictionary of tasks in the workflow\n",
        "\n",
        "    def add_task(self, task_id, instructions, parent_ids=[]):\n",
        "        if task_id not in self.tasks:\n",
        "            self.tasks[task_id] = Task(task_id, instructions, self.id)\n",
        "        task = self.tasks[task_id]\n",
        "        for parent_id in parent_ids:\n",
        "            if parent_id not in self.tasks:\n",
        "                self.tasks[parent_id] = Task(parent_id, 0, self.id)\n",
        "            parent_task = self.tasks[parent_id]\n",
        "            parent_task.children.append(task)\n",
        "            task.parents.append(parent_task)"
      ],
      "metadata": {
        "id": "l1dvEPosHyYn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class parse_dax"
      ],
      "metadata": {
        "id": "sOhKq478H4_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_dax(file_path, workflow_id):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    workflow_id = workflow_id\n",
        "    workflow = Workflow(workflow_id)\n",
        "\n",
        "    # Parse jobs\n",
        "    jobs = {job.attrib['id']: job for job in root.findall('{http://pegasus.isi.edu/schema/DAX}job')}\n",
        "\n",
        "    # Add jobs to workflow\n",
        "    for job_id, job in jobs.items():\n",
        "        instructions = float(job.attrib.get('runtime', 0))\n",
        "        workflow.add_task(job_id, instructions)\n",
        "\n",
        "    # Parse dependencies\n",
        "    for child in root.findall('{http://pegasus.isi.edu/schema/DAX}child'):\n",
        "        child_id = child.attrib['ref']\n",
        "        parent_ids = [parent.attrib['ref'] for parent in child.findall('{http://pegasus.isi.edu/schema/DAX}parent')]\n",
        "        workflow.add_task(child_id, 0, parent_ids)  # Adds a child node with its parent nodes, setting instructions to 0 to avoid overwrite\n",
        "\n",
        "    return workflow\n",
        "\n",
        "\n",
        "def ensemble_of_workflows(name, size=10, distribution='constant', dax_path=''):\n",
        "    ws = []\n",
        "    ensemble = []\n",
        "    directory_path = dax_path  # Directory containing DAX files\n",
        "\n",
        "    # List and filter files in directory\n",
        "    files = os.listdir(directory_path)\n",
        "    filtered_files = [file for file in files if name in file]\n",
        "\n",
        "    if distribution == 'constant':\n",
        "        pattern = r'100(?!\\d)'\n",
        "        for s in filtered_files:\n",
        "            if re.search(pattern, s):\n",
        "                ensemble = [s] * size  # Replicate the matched file 'size' times\n",
        "                break\n",
        "    else:\n",
        "        numbers = np.random.randint(0, len(filtered_files), size)\n",
        "        ensemble = [filtered_files[i] for i in numbers]  # Select random files based on uniform distribution\n",
        "    w_id = 0\n",
        "    for name in ensemble:\n",
        "        ws.append(parse_dax(dax_path+name,w_id))\n",
        "        w_id = w_id + 1\n",
        "\n",
        "    return ws"
      ],
      "metadata": {
        "id": "92rkJc8AH3Bd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dax files"
      ],
      "metadata": {
        "id": "u8oPYhPOIF39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import glob\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/My Drive/Zahra/dax/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z69MgkfSH81N",
        "outputId": "eca7cb77-2167-4dff-af29-fdab15f0ff9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class Device"
      ],
      "metadata": {
        "id": "3x5Y4MN-IMLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Device:\n",
        "    def __init__(self, id, mips, cost_per_hour, env):\n",
        "        self.id = id\n",
        "        self.mips = mips\n",
        "        self.cost_per_hour = cost_per_hour\n",
        "        self.queue = deque()\n",
        "        self.runnig_queue = deque()\n",
        "        self.resource = simpy.Resource(env, capacity=1)\n",
        "\n",
        "    def add_task_to_queue(self, task):\n",
        "        self.queue.append(task)\n",
        "\n",
        "    def get_next_task(self):\n",
        "        return self.queue.popleft() if self.queue else None\n",
        "\n",
        "    def waiting_time(self):\n",
        "        waiting_time = 0\n",
        "        for t in self.queue:\n",
        "            waiting_time += t.instructions / self.mips\n",
        "        return waiting_time\n"
      ],
      "metadata": {
        "id": "kK7fXmQeIPIW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class FogEnv"
      ],
      "metadata": {
        "id": "cUV48C7AImGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import simpy\n",
        "\n",
        "class FogEnv:\n",
        "    def __init__(self, env, iot_devices, fog_nodes, cloud_servers, workflows):\n",
        "        self.env = env\n",
        "        self.iot_devices = iot_devices\n",
        "        self.fog_nodes = fog_nodes\n",
        "        self.cloud_servers = cloud_servers\n",
        "        self.cost = 0\n",
        "        self.completed_workflows = 0\n",
        "        self.workflows = workflows\n",
        "\n",
        "    def assign_task(self, task, device):\n",
        "        with device.resource.request() as req:\n",
        "            yield req\n",
        "            instructions = task.instructions\n",
        "            execution_time = instructions / device.mips\n",
        "            yield self.env.timeout(execution_time)\n",
        "            self.cost += execution_time * device.cost_per_hour\n",
        "            task.executed = True\n",
        "            task.execution_time = execution_time\n",
        "            device.queue.popleft()\n",
        "            print(f\"Task {task.id} of workflow {task.workflow_id} completed on {device.id} at time {self.env.now}\")\n",
        "\n",
        "            # Check if the workflow is completed\n",
        "            workflow = next(wf for wf in self.workflows if wf.id == task.workflow_id)\n",
        "            self.check_workflow_completion(workflow)\n",
        "\n",
        "    def get_state(self, task):\n",
        "        return [self.cost] + [d.waiting_time() for d in self.iot_devices + self.fog_nodes + self.cloud_servers]\n",
        "\n",
        "    def get_device_by_id(self, device_id):\n",
        "        for device in self.iot_devices + self.fog_nodes + self.cloud_servers:\n",
        "            if device.id == device_id:\n",
        "                return device\n",
        "        return None\n",
        "\n",
        "    def check_workflow_completion(self, workflow):\n",
        "        # Check if all tasks in the workflow are executed\n",
        "        if all(task.executed for task in workflow.tasks.values()):\n",
        "            self.completed_workflows += 1  # Increment completed workflows counter\n",
        "            print(f\"Workflow {workflow.id} is completed! Total completed workflows: {self.completed_workflows}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_workflow(env, workflow, fog_env, agent):\n",
        "      while(True):\n",
        "        if all([task.executed for task in workflow.tasks.values()]):\n",
        "          break\n",
        "        for task in workflow.tasks.values():\n",
        "            if task.executed:\n",
        "              continue\n",
        "            if all([parent.executed for parent in task.parents]) or task.parents == []:\n",
        "                state = np.array(fog_env.get_state(task)).reshape(1, -1)\n",
        "                action = agent.choose_action(state)\n",
        "                devices = fog_env.iot_devices + fog_env.fog_nodes + fog_env.cloud_servers\n",
        "                device = devices[action]\n",
        "                device.add_task_to_queue(task)\n",
        "                yield env.process(fog_env.assign_task(task, device))\n",
        "                next_state = np.array(fog_env.get_state(task)).reshape(1, -1)\n",
        "                agent.remember(state, action, -1*fog_env.cost, next_state, False)\n",
        "                agent.replay()"
      ],
      "metadata": {
        "id": "A8B_f_ZNIovb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class NewSim\n",
        "\n"
      ],
      "metadata": {
        "id": "Swj5rrsAJZV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class  NewSim:\n",
        "    def __init__(self, num_iot, num_fog, num_server, learning_rate=0.001, discount_factor=0.95,\n",
        "                 exploration_rate=1.0, exploration_decay=0.995, exploration_min=0.01, batch_size=64, memory_size=2000):\n",
        "        self.num_iot = num_iot\n",
        "        self.num_fog = num_fog\n",
        "        self.num_server = num_server\n",
        "        self.num_totla_dev = num_iot + num_fog + num_server\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_min = exploration_min\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_size = memory_size\n",
        "        self.env = simpy.Environment()\n",
        "        self.reset()\n",
        "        self.run()\n",
        "\n",
        "    def reset(self):\n",
        "        self.iot_devices = [Device(f'iot_{i}', 500, 0, self.env) for i in range(self.num_iot)]\n",
        "        self.fog_devices = [Device(f'fog_{i}', 4000, 1, self.env) for i in range(self.num_fog)]\n",
        "        self.server_devices = [Device(f'server_{i}', 6000, 8, self.env) for i in range(self.num_server)]\n",
        "        self.agent = DQNAgent(state_size=1+self.num_totla_dev, action_size=self.num_totla_dev, learning_rate=self.learning_rate, discount_factor=self.discount_factor,\n",
        "                                  exploration_rate=self.exploration_rate, exploration_decay=self.exploration_decay,\n",
        "                                  exploration_min=self.exploration_min, batch_size=self.batch_size, memory_size=self.memory_size)\n",
        "\n",
        "        self.workflows = ensemble_of_workflows(name = 'CyberShake', size=2, distribution = 'constant', dax_path=\"/content/drive/My Drive/Zahra/dax/\")\n",
        "    def run(self):\n",
        "        fog_env = FogEnv(self.env, self.iot_devices, self.fog_devices, self.server_devices,self.workflows)\n",
        "        for workflow in self.workflows:\n",
        "                self.env.process(process_workflow(self.env, workflow, fog_env, self.agent))\n",
        "\n",
        "        self.env.run()  # Run simulation for a time period\n"
      ],
      "metadata": {
        "id": "SB2sfq22JcBF"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_simulation(num_runs=100):\n",
        "    learning_rate = 0.0001\n",
        "    discount_factor = 0.99\n",
        "    exploration_rate = 0.5\n",
        "    exploration_decay = 0.995\n",
        "    exploration_min = 0.05\n",
        "\n",
        "    best_mean_delay = float('inf')\n",
        "    best_mean_cost = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    simulation = NewSim(num_iot=10, num_fog=8, num_server=5, learning_rate=learning_rate,\n",
        "                        discount_factor=discount_factor, exploration_rate=exploration_rate,\n",
        "                        exploration_decay=exploration_decay, exploration_min=exploration_min)\n",
        "\n",
        "run_simulation(num_runs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aZZe0gGJhOW",
        "outputId": "fee395fd-7a60-4157-f1ba-ef3ad327da81"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task ID00002 of workflow 0 completed on iot_5 at time 0.30889999999999995\n",
            "Task ID00002 of workflow 1 completed on iot_1 at time 0.30889999999999995\n",
            "Task ID00003 of workflow 1 completed on fog_7 at time 0.3226225\n",
            "Task ID00004 of workflow 1 completed on iot_1 at time 0.32482249999999996\n",
            "Task ID00005 of workflow 1 completed on fog_7 at time 0.33851749999999997\n",
            "Task ID00006 of workflow 1 completed on iot_9 at time 0.33967749999999997\n",
            "Task ID00003 of workflow 0 completed on iot_7 at time 0.41867999999999994\n",
            "Task ID00007 of workflow 1 completed on iot_1 at time 0.43757749999999995\n",
            "Task ID00008 of workflow 1 completed on iot_0 at time 0.43889749999999994\n",
            "Task ID00004 of workflow 0 completed on iot_1 at time 0.43977749999999993\n",
            "Task ID00009 of workflow 1 completed on fog_6 at time 0.45187749999999993\n",
            "Task ID00010 of workflow 1 completed on fog_2 at time 0.45206749999999996\n",
            "Task ID00005 of workflow 0 completed on iot_1 at time 0.5493374999999999\n",
            "Task ID00006 of workflow 0 completed on fog_5 at time 0.5494824999999999\n",
            "Task ID00007 of workflow 0 completed on iot_7 at time 0.6473824999999999\n",
            "Task ID00008 of workflow 0 completed on iot_1 at time 0.6487024999999998\n",
            "Task ID00011 of workflow 1 completed on iot_2 at time 0.6983675\n",
            "Task ID00012 of workflow 1 completed on server_1 at time 0.7026525\n",
            "Task ID00013 of workflow 1 completed on iot_0 at time 0.7040325000000001\n",
            "Task ID00009 of workflow 0 completed on iot_1 at time 0.7525424999999999\n",
            "Task ID00010 of workflow 0 completed on server_1 at time 0.7526691666666666\n",
            "Task ID00014 of workflow 1 completed on iot_1 at time 0.8032824999999999\n",
            "Task ID00015 of workflow 1 completed on iot_1 at time 0.8064024999999999\n",
            "Task ID00016 of workflow 1 completed on iot_1 at time 0.8640024999999999\n",
            "Task ID00017 of workflow 1 completed on iot_4 at time 0.8668624999999999\n",
            "Task ID00018 of workflow 1 completed on iot_1 at time 0.9415424999999998\n",
            "Task ID00019 of workflow 1 completed on server_4 at time 0.9417924999999998\n",
            "Task ID00020 of workflow 1 completed on server_3 at time 0.9485141666666664\n",
            "Task ID00021 of workflow 1 completed on iot_3 at time 0.9506941666666664\n",
            "Task ID00011 of workflow 0 completed on iot_8 at time 0.9989691666666667\n",
            "Task ID00012 of workflow 0 completed on fog_4 at time 1.0053966666666667\n",
            "Task ID00013 of workflow 0 completed on iot_1 at time 1.0067766666666667\n",
            "Task ID00022 of workflow 1 completed on iot_2 at time 1.1710941666666663\n",
            "Task ID00014 of workflow 0 completed on iot_2 at time 1.2218341666666663\n",
            "Task ID00015 of workflow 0 completed on iot_1 at time 1.2249541666666663\n",
            "Task ID00016 of workflow 0 completed on fog_6 at time 1.2321541666666664\n",
            "Task ID00017 of workflow 0 completed on iot_7 at time 1.2350141666666665\n",
            "Task ID00018 of workflow 0 completed on iot_1 at time 1.3096941666666666\n",
            "Task ID00023 of workflow 1 completed on iot_2 at time 1.3106341666666663\n",
            "Task ID00024 of workflow 1 completed on fog_1 at time 1.3110266666666663\n",
            "Task ID00019 of workflow 0 completed on iot_7 at time 1.3126941666666665\n",
            "Task ID00025 of workflow 1 completed on server_1 at time 1.3180599999999996\n",
            "Task ID00020 of workflow 0 completed on fog_3 at time 1.3227766666666665\n",
            "Task ID00026 of workflow 1 completed on fog_3 at time 1.3231141666666666\n",
            "Task ID00021 of workflow 0 completed on iot_0 at time 1.3249566666666666\n",
            "Task ID00027 of workflow 1 completed on fog_2 at time 1.3316241666666666\n",
            "Task ID00028 of workflow 1 completed on iot_1 at time 1.3340041666666667\n",
            "Task ID00029 of workflow 1 completed on server_2 at time 1.3432058333333334\n",
            "Task ID00022 of workflow 0 completed on server_1 at time 1.3433233333333332\n",
            "Task ID00030 of workflow 1 completed on fog_4 at time 1.3434033333333335\n",
            "Task ID00023 of workflow 0 completed on server_1 at time 1.3507233333333333\n",
            "Task ID00024 of workflow 0 completed on server_0 at time 1.3509849999999999\n",
            "Task ID00031 of workflow 1 completed on fog_7 at time 1.3539658333333335\n",
            "Task ID00032 of workflow 1 completed on fog_0 at time 1.3543358333333335\n",
            "Task ID00025 of workflow 0 completed on server_1 at time 1.358018333333333\n",
            "Task ID00026 of workflow 0 completed on fog_6 at time 1.3583558333333332\n",
            "Task ID00027 of workflow 0 completed on iot_9 at time 1.4264358333333331\n",
            "Task ID00028 of workflow 0 completed on iot_6 at time 1.4288158333333332\n",
            "Task ID00029 of workflow 0 completed on server_1 at time 1.4380175\n",
            "Task ID00030 of workflow 0 completed on server_1 at time 1.4381491666666666\n",
            "Task ID00031 of workflow 0 completed on server_1 at time 1.4451908333333332\n",
            "Task ID00032 of workflow 0 completed on server_1 at time 1.4454375\n",
            "Task ID00033 of workflow 0 completed on fog_6 at time 1.4664275\n",
            "Task ID00033 of workflow 1 completed on iot_1 at time 1.5222558333333336\n",
            "Task ID00034 of workflow 1 completed on server_1 at time 1.5288625000000002\n",
            "Task ID00035 of workflow 1 completed on iot_0 at time 1.5308625000000002\n",
            "Task ID00036 of workflow 1 completed on server_1 at time 1.5373308333333335\n",
            "Task ID00037 of workflow 1 completed on server_1 at time 1.5375225000000001\n",
            "Task ID00034 of workflow 0 completed on iot_7 at time 1.5457075\n",
            "Task ID00035 of workflow 0 completed on server_1 at time 1.5458741666666667\n",
            "Task ID00036 of workflow 0 completed on server_1 at time 1.5523425\n",
            "Task ID00037 of workflow 0 completed on server_1 at time 1.5525341666666665\n",
            "Task ID00038 of workflow 0 completed on server_1 at time 1.5590758333333332\n",
            "Task ID00039 of workflow 0 completed on iot_3 at time 1.5603758333333333\n",
            "Task ID00040 of workflow 0 completed on server_1 at time 1.5640808333333334\n",
            "Task ID00041 of workflow 0 completed on server_1 at time 1.5642525\n",
            "Task ID00038 of workflow 1 completed on iot_2 at time 1.6160225000000001\n",
            "Task ID00039 of workflow 1 completed on fog_2 at time 1.6161850000000002\n",
            "Task ID00040 of workflow 1 completed on iot_1 at time 1.6606450000000001\n",
            "Task ID00041 of workflow 1 completed on iot_0 at time 1.662705\n",
            "Task ID00042 of workflow 0 completed on iot_2 at time 1.6859625\n",
            "Task ID00043 of workflow 0 completed on server_1 at time 1.6861875\n",
            "Task ID00042 of workflow 1 completed on iot_1 at time 1.7326450000000002\n",
            "Task ID00043 of workflow 1 completed on iot_8 at time 1.7353450000000001\n",
            "Task ID00044 of workflow 1 completed on fog_7 at time 1.7851025\n",
            "Task ID00045 of workflow 1 completed on server_2 at time 1.7892908333333333\n",
            "Task ID00046 of workflow 1 completed on iot_1 at time 1.7906108333333333\n",
            "Task ID00047 of workflow 1 completed on iot_7 at time 1.8438508333333332\n",
            "Task ID00048 of workflow 1 completed on iot_1 at time 1.8457908333333333\n",
            "Task ID00049 of workflow 1 completed on fog_0 at time 1.8553258333333333\n",
            "Task ID00050 of workflow 1 completed on server_0 at time 1.8554491666666666\n",
            "Task ID00051 of workflow 1 completed on iot_1 at time 1.9761491666666666\n",
            "Task ID00052 of workflow 1 completed on iot_1 at time 1.9787491666666666\n",
            "Task ID00053 of workflow 1 completed on iot_1 at time 2.060229166666667\n",
            "Task ID00054 of workflow 1 completed on iot_1 at time 2.0625691666666666\n",
            "Task ID00044 of workflow 0 completed on iot_2 at time 2.0842475\n",
            "Task ID00045 of workflow 0 completed on fog_3 at time 2.09053\n",
            "Task ID00046 of workflow 0 completed on server_1 at time 2.09064\n",
            "Task ID00047 of workflow 0 completed on server_1 at time 2.095076666666667\n",
            "Task ID00048 of workflow 0 completed on iot_4 at time 2.0970166666666668\n",
            "Task ID00049 of workflow 0 completed on fog_1 at time 2.106551666666667\n",
            "Task ID00050 of workflow 0 completed on iot_4 at time 2.1080316666666667\n",
            "Task ID00055 of workflow 1 completed on iot_7 at time 2.1328491666666665\n",
            "Task ID00056 of workflow 1 completed on iot_1 at time 2.1353491666666664\n",
            "Task ID00051 of workflow 0 completed on iot_8 at time 2.2287316666666666\n",
            "Task ID00052 of workflow 0 completed on server_0 at time 2.2289483333333333\n",
            "Task ID00053 of workflow 0 completed on iot_4 at time 2.3104283333333333\n",
            "Task ID00054 of workflow 0 completed on fog_6 at time 2.3107208333333333\n",
            "Task ID00055 of workflow 0 completed on iot_4 at time 2.3810008333333332\n",
            "Task ID00056 of workflow 0 completed on iot_4 at time 2.383500833333333\n",
            "Task ID00057 of workflow 1 completed on iot_8 at time 2.5725516666666666\n",
            "Task ID00057 of workflow 0 completed on iot_4 at time 2.727320833333333\n",
            "Task ID00058 of workflow 0 completed on fog_6 at time 2.732893333333333\n",
            "Task ID00058 of workflow 1 completed on iot_4 at time 2.771900833333333\n",
            "Task ID00059 of workflow 0 completed on iot_4 at time 2.773820833333333\n",
            "Task ID00059 of workflow 1 completed on iot_4 at time 2.7757408333333333\n",
            "Task ID00060 of workflow 0 completed on iot_4 at time 2.855780833333333\n",
            "Task ID00060 of workflow 1 completed on iot_4 at time 2.935820833333333\n",
            "Task ID00061 of workflow 0 completed on iot_4 at time 2.938520833333333\n",
            "Task ID00061 of workflow 1 completed on iot_4 at time 2.941220833333333\n",
            "Task ID00062 of workflow 0 completed on iot_9 at time 3.038420833333333\n",
            "Task ID00062 of workflow 1 completed on iot_4 at time 3.041120833333333\n",
            "Task ID00063 of workflow 0 completed on iot_4 at time 3.043760833333333\n",
            "Task ID00063 of workflow 1 completed on iot_4 at time 3.046400833333333\n",
            "Task ID00064 of workflow 0 completed on iot_4 at time 3.162860833333333\n",
            "Task ID00065 of workflow 0 completed on iot_9 at time 3.1657008333333327\n",
            "Task ID00066 of workflow 0 completed on fog_5 at time 3.1787808333333327\n",
            "Task ID00067 of workflow 0 completed on iot_9 at time 3.1806808333333327\n",
            "Task ID00068 of workflow 0 completed on iot_9 at time 3.232140833333333\n",
            "Task ID00069 of workflow 0 completed on iot_6 at time 3.2341408333333326\n",
            "Task ID00064 of workflow 1 completed on iot_4 at time 3.279320833333333\n",
            "Task ID00065 of workflow 1 completed on server_4 at time 3.2795574999999997\n",
            "Task ID00070 of workflow 0 completed on iot_9 at time 3.6417008333333327\n",
            "Task ID00066 of workflow 1 completed on iot_9 at time 3.7463408333333326\n",
            "Task ID00067 of workflow 1 completed on fog_5 at time 3.7465783333333325\n",
            "Task ID00071 of workflow 0 completed on iot_9 at time 3.8052608333333326\n",
            "Task ID00072 of workflow 0 completed on fog_3 at time 3.8055908333333326\n",
            "Task ID00073 of workflow 0 completed on server_4 at time 3.8132558333333324\n",
            "Task ID00074 of workflow 0 completed on iot_3 at time 3.8154358333333325\n",
            "Task ID00075 of workflow 0 completed on server_4 at time 3.822527499999999\n",
            "Task ID00076 of workflow 0 completed on server_4 at time 3.8226658333333323\n",
            "Task ID00077 of workflow 0 completed on server_4 at time 3.831297499999999\n",
            "Task ID00078 of workflow 0 completed on fog_0 at time 3.831592499999999\n",
            "Task ID00079 of workflow 0 completed on server_4 at time 3.840327499999999\n",
            "Task ID00080 of workflow 0 completed on server_4 at time 3.840494166666666\n",
            "Task ID00068 of workflow 1 completed on iot_9 at time 3.8567208333333327\n",
            "Task ID00069 of workflow 1 completed on server_4 at time 3.8568874999999996\n",
            "Task ID00070 of workflow 1 completed on fog_0 at time 3.9078324999999996\n",
            "Task ID00071 of workflow 1 completed on server_4 at time 3.9127425\n",
            "Task ID00081 of workflow 0 completed on iot_9 at time 3.924740833333333\n",
            "Task ID00082 of workflow 0 completed on iot_0 at time 3.9271608333333328\n",
            "Task ID00072 of workflow 1 completed on iot_9 at time 3.927380833333333\n",
            "Task ID00073 of workflow 1 completed on server_4 at time 3.9350458333333327\n",
            "Task ID00074 of workflow 1 completed on server_4 at time 3.9352274999999994\n",
            "Task ID00083 of workflow 0 completed on fog_2 at time 3.9417108333333326\n",
            "Task ID00075 of workflow 1 completed on server_4 at time 3.942319166666666\n",
            "Task ID00084 of workflow 0 completed on server_4 at time 3.942459166666666\n",
            "Task ID00076 of workflow 1 completed on server_4 at time 3.9425974999999993\n",
            "Task ID00085 of workflow 0 completed on server_4 at time 3.9711858333333327\n",
            "Task ID00077 of workflow 1 completed on server_4 at time 3.9798174999999993\n",
            "Task ID00086 of workflow 0 completed on fog_1 at time 3.9860608333333327\n",
            "Task ID00078 of workflow 1 completed on fog_1 at time 3.9863558333333327\n",
            "Task ID00087 of workflow 0 completed on fog_1 at time 3.986690833333333\n",
            "Task ID00079 of workflow 1 completed on fog_1 at time 3.999793333333333\n",
            "Task ID00088 of workflow 0 completed on fog_1 at time 4.015743333333333\n",
            "Task ID00080 of workflow 1 completed on fog_1 at time 4.015993333333333\n",
            "Task ID00089 of workflow 0 completed on iot_1 at time 4.018063333333333\n",
            "Task ID00081 of workflow 1 completed on fog_5 at time 4.024495833333333\n",
            "Task ID00090 of workflow 0 completed on fog_1 at time 4.033948333333333\n",
            "Task ID00082 of workflow 1 completed on fog_1 at time 4.034250833333333\n",
            "Task ID00091 of workflow 0 completed on fog_1 at time 4.034543333333333\n",
            "Task ID00083 of workflow 1 completed on fog_1 at time 4.0490933333333325\n",
            "Task ID00084 of workflow 1 completed on fog_1 at time 4.049303333333333\n",
            "Task ID00085 of workflow 1 completed on fog_1 at time 4.092185833333333\n",
            "Task ID00092 of workflow 0 completed on iot_1 at time 4.1462233333333325\n",
            "Task ID00093 of workflow 0 completed on fog_1 at time 4.146398333333332\n",
            "Task ID00094 of workflow 0 completed on fog_1 at time 4.158518333333332\n",
            "Task ID00095 of workflow 0 completed on fog_1 at time 4.1587033333333325\n",
            "Task ID00086 of workflow 1 completed on iot_2 at time 4.2111858333333325\n",
            "Task ID00087 of workflow 1 completed on server_4 at time 4.211409166666666\n",
            "Task ID00096 of workflow 0 completed on iot_3 at time 4.272503333333333\n",
            "Task ID00097 of workflow 0 completed on iot_6 at time 4.274463333333333\n",
            "Task ID00098 of workflow 0 completed on server_4 at time 4.282758333333334\n",
            "Task ID00099 of workflow 0 completed on server_1 at time 4.28293\n",
            "Task ID00000 of workflow 0 completed on iot_6 at time 4.28361\n",
            "Task ID00001 of workflow 0 completed on iot_6 at time 4.28521\n",
            "Workflow 0 is completed! Total completed workflows: 1\n",
            "Task ID00088 of workflow 1 completed on iot_8 at time 4.339009166666666\n",
            "Task ID00089 of workflow 1 completed on iot_6 at time 4.341329166666666\n",
            "Task ID00090 of workflow 1 completed on iot_8 at time 4.4684091666666665\n",
            "Task ID00091 of workflow 1 completed on iot_8 at time 4.470749166666667\n",
            "Task ID00092 of workflow 1 completed on iot_5 at time 4.5824291666666666\n",
            "Task ID00093 of workflow 1 completed on fog_3 at time 4.582604166666666\n",
            "Task ID00094 of workflow 1 completed on iot_8 at time 4.679564166666666\n",
            "Task ID00095 of workflow 1 completed on iot_5 at time 4.681044166666666\n",
            "Task ID00096 of workflow 1 completed on fog_2 at time 4.695269166666666\n",
            "Task ID00097 of workflow 1 completed on iot_8 at time 4.697229166666666\n",
            "Task ID00098 of workflow 1 completed on iot_8 at time 4.7967691666666665\n",
            "Task ID00099 of workflow 1 completed on iot_8 at time 4.798829166666667\n",
            "Task ID00000 of workflow 1 completed on iot_8 at time 4.799509166666667\n",
            "Task ID00001 of workflow 1 completed on iot_8 at time 4.8011091666666665\n",
            "Workflow 1 is completed! Total completed workflows: 2\n"
          ]
        }
      ]
    }
  ]
}