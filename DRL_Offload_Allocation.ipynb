{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkMWL+0TpdTaoNre5oOm6I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zahra-FallahMMA/DRL_Offload_Allocation/blob/main/DRL_Offload_Allocation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import Libraries"
      ],
      "metadata": {
        "id": "PcuUUZU1G5k4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIP4GlsbGwAx",
        "outputId": "b0a30a63-f785-4ee2-d05f-a795976cbefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simpy\n",
            "  Downloading simpy-4.1.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "Downloading simpy-4.1.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: simpy\n",
            "Successfully installed simpy-4.1.1\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from io import StringIO\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import deque, defaultdict\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from itertools import product\n",
        "import tensorflow as tf\n",
        "!pip install simpy\n",
        "import simpy\n",
        "\n",
        "# Set TensorFlow logging level to suppress detailed logs\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class ReplayBuffer"
      ],
      "metadata": {
        "id": "n3fKTuuQHDo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.discrete = discrete\n",
        "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
        "        dtype = np.int8 if self.discrete else np.float32\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype)\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        if self.discrete:\n",
        "            actions = np.zeros(self.action_memory.shape[1])\n",
        "            actions[action] = 1.0\n",
        "            self.action_memory[index] = actions\n",
        "        else:\n",
        "            self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = 1 - done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size)\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "        return states, actions, rewards, states_, terminal\n"
      ],
      "metadata": {
        "id": "SRdAlbgAG3w8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class DQNAgent"
      ],
      "metadata": {
        "id": "lCfIwrJDHK33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, learning_rate=0.001, discount_factor=0.95, exploration_rate=1.0,\n",
        "                 exploration_decay=0.995, exploration_min=0.01, batch_size=64, memory_size=2000, model_path = None):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_min = exploration_min\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = ReplayBuffer(memory_size, state_size, action_size, discrete=True)\n",
        "        if model_path is not None:\n",
        "            self.model = load_model(model_path)\n",
        "            self.target_model = load_model(model_path)\n",
        "        else:\n",
        "            self.model = self._build_model()\n",
        "            self.target_model = self._build_model()\n",
        "\n",
        "\n",
        "        self.update_target_model()\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Input layer\n",
        "        model.add(Dense(128, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dropout(0.4))  # Slightly reduced dropout to retain more information\n",
        "\n",
        "        # Hidden layers\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "\n",
        "        # Compile the model with a custom learning rate scheduler\n",
        "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() <= self.exploration_rate:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = np.array(state).reshape(1, -1)  # Ensure state is 2D\n",
        "        q_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        targets = self.model.predict(states, verbose=0)\n",
        "        target_next = self.target_model.predict(next_states, verbose=0)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            action_index = np.argmax(actions[i])  # Find the index of the action\n",
        "            if dones[i]:\n",
        "                targets[i, action_index] = rewards[i]\n",
        "            else:\n",
        "                targets[i, action_index] = rewards[i] + self.discount_factor * np.amax(target_next[i])\n",
        "\n",
        "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "        if self.exploration_rate > self.exploration_min:\n",
        "            self.exploration_rate *= self.exploration_decay\n",
        "\n",
        "        # Update target model every 10 episodes or steps\n",
        "        self.target_update_counter += 1\n",
        "        if self.target_update_counter % 10 == 0:\n",
        "            self.update_target_model()\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "        #if self.target_update_counter % 100 == 0:  # Save every 50 updates\n",
        "         #  self.save_model(\"dqn_checkpoint.keras\")\n",
        "\n",
        "\n",
        "    def save_model(self, path):\n",
        "        self.model.save(path)"
      ],
      "metadata": {
        "id": "qreTNfv_HP9X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class Task"
      ],
      "metadata": {
        "id": "sc5cwMB6HUAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Task:\n",
        "    def __init__(self, id, instructions, workflow_id):\n",
        "        self.id = id\n",
        "        self.instructions = instructions  # Execution time or computational instructions\n",
        "        self.children = []  # List of tasks that depend on this task\n",
        "        self.parents = []  # List of tasks this task depends on\n",
        "        self.executed = False  # Status of execution\n",
        "        self.executed_on = None  # Node this task was executed on\n",
        "        self.execution_time = 0  # Time taken to execute the task\n",
        "        self.cost = 0  # Cost of executing the task\n",
        "        self.comm_delay = 0  # Communication delay in seconds\n",
        "        self.workflow_id = workflow_id  # Workflow identifier to which this task belongs\n"
      ],
      "metadata": {
        "id": "dWqg63i_HW5y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Workflow"
      ],
      "metadata": {
        "id": "V6GeROhDHben"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Workflow:\n",
        "    def __init__(self, id):\n",
        "        self.id = id  # Workflow identifier\n",
        "        self.tasks = {}  # Dictionary of tasks in the workflow\n",
        "\n",
        "    def add_task(self, task_id, instructions, parent_ids=[]):\n",
        "        if task_id not in self.tasks:\n",
        "            self.tasks[task_id] = Task(task_id, instructions, self.id)\n",
        "        task = self.tasks[task_id]\n",
        "        for parent_id in parent_ids:\n",
        "            if parent_id not in self.tasks:\n",
        "                self.tasks[parent_id] = Task(parent_id, 0, self.id)\n",
        "            parent_task = self.tasks[parent_id]\n",
        "            parent_task.children.append(task)\n",
        "            task.parents.append(parent_task)"
      ],
      "metadata": {
        "id": "l1dvEPosHyYn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class parse_dax"
      ],
      "metadata": {
        "id": "sOhKq478H4_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_dax(file_path, workflow_id):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    workflow_id = workflow_id\n",
        "    workflow = Workflow(workflow_id)\n",
        "\n",
        "    # Parse jobs\n",
        "    jobs = {job.attrib['id']: job for job in root.findall('{http://pegasus.isi.edu/schema/DAX}job')}\n",
        "\n",
        "    # Add jobs to workflow\n",
        "    for job_id, job in jobs.items():\n",
        "        instructions = float(job.attrib.get('runtime', 0))\n",
        "        workflow.add_task(job_id, instructions)\n",
        "\n",
        "    # Parse dependencies\n",
        "    for child in root.findall('{http://pegasus.isi.edu/schema/DAX}child'):\n",
        "        child_id = child.attrib['ref']\n",
        "        parent_ids = [parent.attrib['ref'] for parent in child.findall('{http://pegasus.isi.edu/schema/DAX}parent')]\n",
        "        workflow.add_task(child_id, 0, parent_ids)  # Adds a child node with its parent nodes, setting instructions to 0 to avoid overwrite\n",
        "\n",
        "    return workflow\n",
        "\n",
        "\n",
        "def ensemble_of_workflows(name, size=10, distribution='constant', dax_path=''):\n",
        "    ws = []\n",
        "    ensemble = []\n",
        "    directory_path = dax_path  # Directory containing DAX files\n",
        "\n",
        "    # List and filter files in directory\n",
        "    files = os.listdir(directory_path)\n",
        "    filtered_files = [file for file in files if name in file]\n",
        "\n",
        "    if distribution == 'constant':\n",
        "        pattern = r'100(?!\\d)'\n",
        "        for s in filtered_files:\n",
        "            if re.search(pattern, s):\n",
        "                ensemble = [s] * size  # Replicate the matched file 'size' times\n",
        "                break\n",
        "    else:\n",
        "        numbers = np.random.randint(0, len(filtered_files), size)\n",
        "        ensemble = [filtered_files[i] for i in numbers]  # Select random files based on uniform distribution\n",
        "    w_id = 0\n",
        "    for name in ensemble:\n",
        "        ws.append(parse_dax(dax_path+name,w_id))\n",
        "        w_id = w_id + 1\n",
        "\n",
        "    return ws"
      ],
      "metadata": {
        "id": "92rkJc8AH3Bd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dax files"
      ],
      "metadata": {
        "id": "u8oPYhPOIF39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import glob\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/My Drive/Zahra/dax/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z69MgkfSH81N",
        "outputId": "4ce4c9af-6124-492b-97ae-f3834f5a73bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class Device"
      ],
      "metadata": {
        "id": "3x5Y4MN-IMLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Device:\n",
        "    def __init__(self, id, mips, cost_per_hour, env):\n",
        "        self.id = id\n",
        "        self.mips = mips\n",
        "        self.cost_per_hour = cost_per_hour\n",
        "        self.queue = deque()\n",
        "        self.runnig_queue = deque()\n",
        "        self.resource = simpy.Resource(env, capacity=1)\n",
        "\n",
        "    def add_task_to_queue(self, task):\n",
        "        self.queue.append(task)\n",
        "\n",
        "    def get_next_task(self):\n",
        "        return self.queue.popleft() if self.queue else None\n",
        "\n",
        "    def waiting_time(self):\n",
        "        waiting_time = 0\n",
        "        for t in self.queue:\n",
        "            waiting_time += t.instructions / self.mips\n",
        "        return waiting_time\n"
      ],
      "metadata": {
        "id": "kK7fXmQeIPIW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class FogEnv"
      ],
      "metadata": {
        "id": "cUV48C7AImGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import simpy\n",
        "\n",
        "class FogEnv:\n",
        "    def __init__(self, env, iot_devices, fog_nodes, cloud_servers, workflows):\n",
        "        self.env = env\n",
        "        self.iot_devices = iot_devices\n",
        "        self.fog_nodes = fog_nodes\n",
        "        self.cloud_servers = cloud_servers\n",
        "        self.cost = 0\n",
        "        self.completed_workflows = 0\n",
        "        self.workflows = workflows\n",
        "\n",
        "    def assign_task(self, task, device):\n",
        "        with device.resource.request() as req:\n",
        "            yield req\n",
        "            instructions = task.instructions\n",
        "            execution_time = instructions / device.mips\n",
        "            yield self.env.timeout(execution_time)\n",
        "            self.cost += execution_time * device.cost_per_hour\n",
        "            task.executed = True\n",
        "            task.execution_time = execution_time\n",
        "            device.queue.popleft()\n",
        "            # print(f\"Task {task.id} of workflow {task.workflow_id} completed on {device.id} at time {self.env.now}\")\n",
        "\n",
        "            # Check if the workflow is completed\n",
        "            workflow = next(wf for wf in self.workflows if wf.id == task.workflow_id)\n",
        "            self.check_workflow_completion(workflow)\n",
        "\n",
        "    def get_state(self, task):\n",
        "        return [self.cost] + [d.waiting_time() for d in self.iot_devices + self.fog_nodes + self.cloud_servers]\n",
        "\n",
        "    def get_device_by_id(self, device_id):\n",
        "        for device in self.iot_devices + self.fog_nodes + self.cloud_servers:\n",
        "            if device.id == device_id:\n",
        "                return device\n",
        "        return None\n",
        "\n",
        "    def check_workflow_completion(self, workflow):\n",
        "        # Check if all tasks in the workflow are executed\n",
        "        if all(task.executed for task in workflow.tasks.values()):\n",
        "            self.completed_workflows += 1  # Increment completed workflows counter\n",
        "            # print(f\"Workflow {workflow.id} is completed! Total completed workflows: {self.completed_workflows}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_workflow(env, workflow, fog_env, agent):\n",
        "      while(True):\n",
        "        if all([task.executed for task in workflow.tasks.values()]):\n",
        "          break\n",
        "        for task in workflow.tasks.values():\n",
        "            if task.executed:\n",
        "              continue\n",
        "            if all([parent.executed for parent in task.parents]) or task.parents == []:\n",
        "                state = np.array(fog_env.get_state(task)).reshape(1, -1)\n",
        "                action = agent.choose_action(state)\n",
        "                devices = fog_env.iot_devices + fog_env.fog_nodes + fog_env.cloud_servers\n",
        "                device = devices[action]\n",
        "                device.add_task_to_queue(task)\n",
        "                yield env.process(fog_env.assign_task(task, device))\n",
        "                next_state = np.array(fog_env.get_state(task)).reshape(1, -1)\n",
        "                agent.remember(state, action, fog_env.completed_workflows/(0.00000001+fog_env.cost) , next_state, False)\n",
        "                agent.replay()"
      ],
      "metadata": {
        "id": "A8B_f_ZNIovb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class NewSim\n",
        "\n"
      ],
      "metadata": {
        "id": "Swj5rrsAJZV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class  NewSim:\n",
        "    def __init__(self, num_iot, num_fog, num_server, learning_rate=0.001, discount_factor=0.95,\n",
        "                 exploration_rate=1.0, exploration_decay=0.995, exploration_min=0.01, batch_size=64, memory_size=2000, model_path=None):\n",
        "        self.num_iot = num_iot\n",
        "        self.num_fog = num_fog\n",
        "        self.num_server = num_server\n",
        "        self.num_totla_dev = num_iot + num_fog + num_server\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_min = exploration_min\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_size = memory_size\n",
        "        self.model_path = model_path\n",
        "        self.env = simpy.Environment()\n",
        "        self.reset()\n",
        "        self.run()\n",
        "\n",
        "    def reset(self):\n",
        "        self.iot_devices = [Device(f'iot_{i}', 500, 0, self.env) for i in range(self.num_iot)]\n",
        "        self.fog_devices = [Device(f'fog_{i}', 4000, 1, self.env) for i in range(self.num_fog)]\n",
        "        self.server_devices = [Device(f'server_{i}', 6000, 8, self.env) for i in range(self.num_server)]\n",
        "        self.agent = DQNAgent(state_size=1+self.num_totla_dev, action_size=self.num_totla_dev, learning_rate=self.learning_rate, discount_factor=self.discount_factor,\n",
        "                                  exploration_rate=self.exploration_rate, exploration_decay=self.exploration_decay,\n",
        "                                  exploration_min=self.exploration_min, batch_size=self.batch_size, memory_size=self.memory_size, model_path = self.model_path)\n",
        "\n",
        "        self.workflows = ensemble_of_workflows(name = 'CyberShake', size=100, distribution = 'constant', dax_path=\"/content/drive/My Drive/Zahra/dax/\")\n",
        "    def run(self):\n",
        "        self.fog_env = FogEnv(self.env, self.iot_devices, self.fog_devices, self.server_devices,self.workflows)\n",
        "        for workflow in self.workflows:\n",
        "            self.env.process(process_workflow(self.env, workflow, self.fog_env, self.agent))\n",
        "\n",
        "        self.env.run()  # Run simulation for a time period\n"
      ],
      "metadata": {
        "id": "SB2sfq22JcBF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection\n"
      ],
      "metadata": {
        "id": "8m16rQswH07A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Config of Workflows"
      ],
      "metadata": {
        "id": "pRiUawjwavRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Workflow configurations with specific sizes\n",
        "workflow_configs = {\n",
        "        'CyberShake': [30, 50, 100, 1000],\n",
        "        'Montage': [20, 40, 60, 80, 100, 200, 300],\n",
        "        'Inspiral': [30, 50, 100, 1000],\n",
        "        'Sipht': [29, 58, 100, 968]\n",
        "    }\n",
        "workflow_distributions = ['constant', 'uniform']"
      ],
      "metadata": {
        "id": "pi6ASMtxau1Y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_simulation_with_results_tracking(workflow_name, workflow_size, workflow_distribution, model_path):\n",
        "    learning_rate = 0.001\n",
        "    discount_factor = 0.99\n",
        "    exploration_rate = 0.5\n",
        "    exploration_decay = 0.995\n",
        "    exploration_min = 0.05\n",
        "\n",
        "    dax_path = \"/content/drive/My Drive/Zahra/dax/\"\n",
        "\n",
        "    print(f\"Running simulation for Workflow: {workflow_name}, Size: {workflow_size}, Distribution: {workflow_distribution}\")\n",
        "\n",
        "    # Set up the simulation with the current parameters\n",
        "    simulation = NewSim(\n",
        "        num_iot=10,\n",
        "        num_fog=8,\n",
        "        num_server=5,\n",
        "        learning_rate=learning_rate,\n",
        "        discount_factor=discount_factor,\n",
        "        exploration_rate=exploration_rate,\n",
        "        exploration_decay=exploration_decay,\n",
        "        exploration_min=exploration_min,\n",
        "        model_path = model_path\n",
        "    )\n",
        "\n",
        "    # Update the workflow parameters\n",
        "    simulation.workflows = ensemble_of_workflows(\n",
        "        name=workflow_name,\n",
        "        size=workflow_size,\n",
        "        distribution=workflow_distribution,\n",
        "        dax_path=dax_path\n",
        "    )\n",
        "\n",
        "    # Run the simulation\n",
        "    simulation.run()\n",
        "    drive_path = '/content/drive/My Drive/Zahra/Models/'\n",
        "    model_filename = f\"model_{workflow_name}_{workflow_distribution}_{workflow_size}.keras\"\n",
        "    model_save_path = drive_path + model_filename\n",
        "    simulation.agent.save_model(model_save_path)\n",
        "\n",
        "\n",
        "    # Print results for current run\n",
        "    print(f\"Total cost for Workflow {workflow_name} ({workflow_distribution}, size={workflow_size}): {simulation.fog_env.cost}\")\n",
        "    print(f\"Total time for Workflow {workflow_name} ({workflow_distribution}, size={workflow_size}): {simulation.env.now}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BrXkI2OeLjOO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result for CyberShake"
      ],
      "metadata": {
        "id": "Tl1wA01Rg2sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_simulation_with_results_tracking('CyberShake',10, 'constant', None )\n",
        "run_simulation_with_results_tracking('CyberShake',10, 'uniform', None )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BLu9xLQLwbQ",
        "outputId": "24b04d09-8fa4-4c10-e1c5-974d697d9836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running simulation for Workflow: CyberShake, Size: 10, Distribution: constant\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total cost for Workflow CyberShake (constant, size=10): 0.5053325000000001\n",
            "Total time for Workflow CyberShake (constant, size=10): 154.5018399999997\n",
            "Running simulation for Workflow: CyberShake, Size: 10, Distribution: uniform\n",
            "Total cost for Workflow CyberShake (uniform, size=10): 16.361176666666676\n",
            "Total time for Workflow CyberShake (uniform, size=10): 103.59742083333373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_simulation_with_results_tracking('CyberShake',20, 'constant','/content/drive/My Drive/Zahra/Models/model_CyberShake_uniform_20.keras' )"
      ],
      "metadata": {
        "id": "q8vtR3rRaiB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2765dd-5e10-4b34-b7e6-871fde4eb6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running simulation for Workflow: CyberShake, Size: 20, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=20): 1.4720949999999997\n",
            "Total time for Workflow CyberShake (constant, size=20): 216.83032416666416\n",
            "Running simulation for Workflow: CyberShake, Size: 20, Distribution: uniform\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_simulation_with_results_tracking('CyberShake',20, 'uniform', '/content/drive/My Drive/Zahra/Models/model_CyberShake_constant_20.keras' )"
      ],
      "metadata": {
        "id": "G3FRU4P8Qlxp",
        "outputId": "44301c75-d80b-4667-f37e-b4000beccf09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running simulation for Workflow: CyberShake, Size: 20, Distribution: uniform\n",
            "Total cost for Workflow CyberShake (uniform, size=20): 2.86786\n",
            "Total time for Workflow CyberShake (uniform, size=20): 846.9145400000252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_simulation_with_results_tracking('CyberShake',30, 'constant','/content/drive/My Drive/Zahra/Models/model_CyberShake_uniform_20.keras' )"
      ],
      "metadata": {
        "id": "qvs6flinlbt7",
        "outputId": "c3b3ed75-4e63-4df8-a0f5-cb265ee042e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running simulation for Workflow: CyberShake, Size: 30, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=30): 60.6877166666669\n",
            "Total time for Workflow CyberShake (constant, size=30): 585.8633508333326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_simulation_with_results_tracking('CyberShake',30, 'uniform', '/content/drive/My Drive/Zahra/Models/model_CyberShake_constant_30.keras' )# loss = -cost + 10*completed_workflows"
      ],
      "metadata": {
        "id": "904lLclG8KjK",
        "outputId": "340dfa45-e140-456d-f74b-c2136fa701b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running simulation for Workflow: CyberShake, Size: 30, Distribution: uniform\n",
            "Total cost for Workflow CyberShake (uniform, size=30): 68.2928075000002\n",
            "Total time for Workflow CyberShake (uniform, size=30): 227.7689733333362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_simulation_with_results_tracking('CyberShake',40, 'constant', '/content/drive/My Drive/Zahra/Models/model_CyberShake_uniform_30.keras' )# loss = - cost"
      ],
      "metadata": {
        "id": "ydXijijsU3T4",
        "outputId": "2db9e5b0-7d30-40a3-9e1e-1c7ebd6cf12f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running simulation for Workflow: CyberShake, Size: 40, Distribution: constant\n",
            "Total cost for Workflow CyberShake (constant, size=40): 2.202583333333334\n",
            "Total time for Workflow CyberShake (constant, size=40): 366.84698249999553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_simulation_with_results_tracking('CyberShake',40, 'uniform', '/content/drive/My Drive/Zahra/Models/model_CyberShake_constant_40.keras' ) # loss = - cost"
      ],
      "metadata": {
        "id": "_N74raxBkm-c",
        "outputId": "c8fe172f-6a23-4062-e708-8773280517b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running simulation for Workflow: CyberShake, Size: 40, Distribution: uniform\n",
            "Total cost for Workflow CyberShake (uniform, size=40): 4.5266899999999985\n",
            "Total time for Workflow CyberShake (uniform, size=40): 1082.0054200000377\n"
          ]
        }
      ]
    }
  ]
}